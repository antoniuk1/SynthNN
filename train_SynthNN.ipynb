{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a686162",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Initial Training\n",
      "0\n",
      "[0.4345703, 0.44050348, 1398, 27304, 20895, 1011]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/pymatgen/core/periodic_table.py:212: UserWarning: No electronegativity for Ne. Setting to NaN. This has no physical meaning, and is mainly done to avoid errors caused by the code expecting a float.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from io import StringIO \n",
    "import re\n",
    "import pymatgen as mg\n",
    "import tensorflow as tf\n",
    "import random\n",
    "import os\n",
    "from pymatgen.core.periodic_table import Element\n",
    "import linecache\n",
    "\n",
    "def load_data(data, is_charge_balanced, max_atoms=5, max_coefficient=100000):\n",
    "    #takes input file (icsd_full_properties_no_frac_charges) and processes the data and applies some filters\n",
    "    output_array=[]\n",
    "    coeff_array=np.zeros((10000,1))\n",
    "    element_names_array=['H','He','Li','Be','B','C','N','O','F','Ne','Na','Mg','Al','Si','P','S','Cl', 'Ar','K','Ca','Sc','Ti','V','Cr','Mn','Fe','Co','Ni','Cu','Zn','Ga','Ge','As','Se','Br','Kr','Rb','Sr','Y','Zr','Nb','Mo','Tc','Ru','Rh','Pd','Ag','Cd','In','Sn','Sb','Te','I','Xe','Cs','Ba','La','Ce','Pr','Nd','Pm','Sm','Eu','Gd','Tb','Dy','Ho','Er','Tm','Yb','Lu','Hf','Ta','W','Re','Os','Ir','Pt','Au','Hg','Tl','Pb','Bi','Po','At','Rn','Fr','Ra','Ac','Th','Pa','U','Np','Pu']\n",
    "    for i in range(len(data)):\n",
    "        try:\n",
    "            comp=mg.core.composition.Composition(data[i,1])\n",
    "        except:\n",
    "            continue #bad formula  \n",
    "        if(len(mg.core.composition.Composition(data[i,1]))==1):\n",
    "            continue   \n",
    "        truth_array=[]\n",
    "        for element_name in mg.core.composition.Composition(data[i,1]).as_dict().keys():\n",
    "            if(element_name not in element_names_array):\n",
    "                truth_array.append('False')\n",
    "        if('False' in truth_array):\n",
    "            continue   \n",
    "        if(is_charge_balanced):\n",
    "            if('True' in data[i][8]):\n",
    "                if(len(mg.core.composition.Composition(data[i,1]))<max_atoms):\n",
    "                    values=mg.core.composition.Composition(data[i,1]).as_dict().values()\n",
    "                    for value in values:\n",
    "                        coeff_array[int(value)]=coeff_array[int(value)]+1\n",
    "                    large_values=[x for x in values if x>max_coefficient]\n",
    "                    if(len(large_values)==0):\n",
    "                        output_array.append(mg.core.composition.Composition(data[i,1]).alphabetical_formula.replace(' ', ''))\n",
    "        else:\n",
    "            output_array.append(mg.core.composition.Composition(data[i,1]).alphabetical_formula.replace(' ', ''))\n",
    "    return(np.unique(output_array))\n",
    "\n",
    "def get_features(data0):\n",
    "    p = re.compile('[A-Z][a-z]?\\d*\\.?\\d*')\n",
    "    p3 = re.compile('[A-Z][a-z]?')\n",
    "    p5 = re.compile('\\d+\\.?\\d+|\\d+')\n",
    "    data0_ratio=[]\n",
    "    for i in data0:\n",
    "        \n",
    "        x = mg.core.composition.Composition(i).alphabetical_formula.replace(' ','')\n",
    "        p2 = p.findall(x)\n",
    "        temp1,temp2 = [], []\n",
    "        for x in p2:\n",
    "            temp1.append(Element[p3.findall(x)[0]].number)    \n",
    "            kkk = p5.findall(x)\n",
    "            if len(kkk)<1:\n",
    "                temp2.append(1)\n",
    "            else:\n",
    "                temp2.append(kkk[0])\n",
    "        data0_ratio.append([temp1,list(map(float,temp2))])\n",
    "\n",
    "    I = 94\n",
    "    featmat0 = np.zeros((len(data0_ratio),I))\n",
    "    # featmat: n-hot vectors with fractions\n",
    "    for idx,ent in enumerate(data0_ratio):\n",
    "        for idy,at in enumerate(ent[0]):\n",
    "            featmat0[idx,at-1] = ent[1][idy]/sum(ent[1])\n",
    "    return(featmat0)\n",
    "\n",
    "def random_lines(filename, file_size, num_samples):\n",
    "    idxs = random.sample(range(1,file_size), num_samples)\n",
    "    return ([linecache.getline(filename, i) for i in idxs], idxs)\n",
    "\n",
    "def get_batch_val(neg_positive_ratio):\n",
    "    random.seed(3)\n",
    "    np.random.seed(3)\n",
    "    noTr_positives=48200 #number positive examples in train set\n",
    "    noTr_negatives=noTr_positives*neg_positive_ratio #no. negatives examples in train set\n",
    "    noTr = noTr_positives + (noTr_negatives) #total size of train set\n",
    "    #only sample from first 90% of dataset ( need to shuffle first because GNN_icsd is alphabetical!)\n",
    "    data1=[]\n",
    "    f=open('icsd_full_data_unique_no_frac_no_penta_2020.txt')\n",
    "    i=0\n",
    "    for line in f:\n",
    "        if(i>noTr_positives and i<noTr_positives*1.05):\n",
    "            data1.append(line.replace('\\n',''))\n",
    "        i+=1\n",
    "    f.close()\n",
    "\n",
    "    data0=[]\n",
    "    f=open('standard_neg_ex_tr_val_v5_balanced_shuffled.txt')\n",
    "    i=0\n",
    "    for line in f:\n",
    "        if(i>noTr_negatives and i<noTr_negatives*1.05):\n",
    "            data0.append(line.replace('\\n',''))\n",
    "        i+=1\n",
    "    f.close()\n",
    "    #shuffle the positive and negative examples with themselves\n",
    "    negative_indices=list(range(0,len(data0)))\n",
    "    random.shuffle(negative_indices)\n",
    "    positive_indices=list(range(0,len(data1)))\n",
    "    random.shuffle(positive_indices)\n",
    "    data0=np.array(data0)\n",
    "    data1=np.array(data1)\n",
    "\n",
    "    data0=data0[negative_indices]\n",
    "    data1=data1[positive_indices]\n",
    "    featmat0=get_features(data0)\n",
    "    featmat1=get_features(data1)\n",
    "\n",
    "    #get labels\n",
    "    labs = np.zeros((len(data0) + len(data1),1))\n",
    "    for ind,ent in enumerate(data1):\n",
    "        labs[ind,0] = 1        \n",
    "    unique, counts = np.unique(labs, return_counts=True)\n",
    "    ind0 = np.where(labs==0)[0] #indices of label=0\n",
    "    ind1 = np.where(labs==1)[0] #indices of label=1\n",
    "    #print(len(ind0),len(ind1))\n",
    "\n",
    "    #combine positives and negatives and shuffle\n",
    "\n",
    "    featmat3 = np.concatenate((featmat0,featmat1)) #set legths of labels 0 and 1 to be the same in the new feature matrix featmat3\n",
    "    datasorted = np.concatenate((data0,data1)) #data ordered the same as featmat3\n",
    "    labs3 = np.concatenate((labs[ind0], labs[ind1]), axis=0) #labels ordered the same as featmat3\n",
    "\n",
    "    noS = len(featmat3)\n",
    "    ind = list(range(0,noS)) #training set index\n",
    "    random.shuffle(ind) #shuffle training set index\n",
    "    #indB = list(range(0,noTr_subset)) #used later for batch\n",
    "    labs3 = np.column_stack((labs3,np.abs(labs3-1)))\n",
    "\n",
    "    xtr_batch = featmat3[ind[0:],:]\n",
    "    ytr_batch = labs3[ind[0:],:]\n",
    "    data_batch=datasorted[ind[0:]]\n",
    "    return(xtr_batch, ytr_batch, data_batch)\n",
    "\n",
    "\n",
    "def get_batch(batch_size, neg_positive_ratio, use_semi_weights, model_name, seed=False, seed_value=0):\n",
    "    def random_lines(filename, file_size, num_samples):\n",
    "        idxs = random.sample(range(1,file_size), num_samples)\n",
    "        return ([linecache.getline(filename, i) for i in idxs], idxs)\n",
    "    if(seed):\n",
    "        random.seed(seed_value)\n",
    "        np.random.seed(seed_value)\n",
    "    else:\n",
    "        random.seed()\n",
    "        np.random.seed() \n",
    "    num_positive_examples=int(np.floor(batch_size*(1/(1+neg_positive_ratio))))\n",
    "    num_negative_examples=batch_size-num_positive_examples\n",
    "    noTr_positives=48200 #number positive examples in train set\n",
    "    noTr_negatives=noTr_positives*neg_positive_ratio #no. negatives examples in train set\n",
    "    noTr = noTr_positives + (noTr_negatives) #total size of train set\n",
    "    #only sample from first 90% of dataset ( need to shuffle first because GNN_icsd is alphabetical!)\n",
    "    data1=[]\n",
    "    pulled_lines1,idxs1=random_lines('icsd_full_data_unique_no_frac_no_penta_2020.txt', noTr_positives,num_positive_examples)  \n",
    "    for line in pulled_lines1:\n",
    "        data1.append(line.replace('\\n',''))\n",
    "    data0=[]\n",
    "    pulled_lines0,idxs0=random_lines('standard_neg_ex_tr_val_v5_balanced_shuffled.txt', noTr_negatives, num_negative_examples)\n",
    "    for line in pulled_lines0:\n",
    "        data0.append(line.replace('\\n',''))\n",
    "    \n",
    "    #do consistent shuffling once examples have been chosen\n",
    "    random.seed(3)\n",
    "    np.random.seed(3)\n",
    "    #shuffle the positive and negative examples with themselves\n",
    "    negative_indices=list(range(0,len(data0)))\n",
    "    random.shuffle(negative_indices)\n",
    "    positive_indices=list(range(0,len(data1)))\n",
    "    random.shuffle(positive_indices)\n",
    "    data0=np.array(data0)\n",
    "    data1=np.array(data1)\n",
    "    idxs0=np.array(idxs0)\n",
    "    idxs1=np.array(idxs1)\n",
    "    data0=data0[negative_indices]\n",
    "    data1=data1[positive_indices]\n",
    "    featmat0=get_features(data0)\n",
    "    featmat1=get_features(data1)\n",
    "    idxs0=idxs0[negative_indices]\n",
    "    idxs1=idxs1[positive_indices]\n",
    "    \n",
    "    #get labels\n",
    "    labs = np.zeros((len(data0) + len(data1),1))\n",
    "    for ind,ent in enumerate(data1):\n",
    "        labs[ind,0] = 1        \n",
    "    unique, counts = np.unique(labs, return_counts=True)\n",
    "    ind0 = np.where(labs==0)[0] #indices of label=0\n",
    "    ind1 = np.where(labs==1)[0] #indices of label=1\n",
    "    #print(len(ind0),len(ind1))\n",
    "\n",
    "    #combine positives and negatives and shuffle\n",
    "    featmat3 = np.concatenate((featmat0,featmat1)) #set legths of labels 0 and 1 to be the same in the new feature matrix featmat3\n",
    "    datasorted = np.concatenate((data0,data1)) #data ordered the same as featmat3\n",
    "    labs3 = np.concatenate((labs[ind0], labs[ind1]), axis=0) #labels ordered the same as featmat3\n",
    "    idxs_full=np.concatenate((idxs0,idxs1))  \n",
    "    noS = len(featmat3)\n",
    "    ind = list(range(0,noS)) #training set index\n",
    "    random.shuffle(ind) #shuffle training set index\n",
    "    labs3 = np.column_stack((labs3,np.abs(labs3-1)))\n",
    "    xtr_batch = featmat3[ind[0:],:]\n",
    "    ytr_batch = labs3[ind[0:],:]\n",
    "    data_batch=datasorted[ind[0:]]\n",
    "    idxs_full=idxs_full[ind[0:]]  \n",
    "    #all weights stuff here\n",
    "    weights_full=[]\n",
    "    if(use_semi_weights):\n",
    "        weights1=[]\n",
    "        file=open('semi_weights_testing_pos_30M' + model_name + '.txt','r')\n",
    "        content=file.readlines()\n",
    "        weights1=[]\n",
    "        for i in idxs1:\n",
    "            weights1.append(float(content[i-1].split()[1]))\n",
    "        file.close()\n",
    "\n",
    "        weights0=[]\n",
    "        file=open('semi_weights_testing_neg_30M' + model_name + '.txt','r')\n",
    "        content=file.readlines()\n",
    "        for i in idxs0:\n",
    "            weights0.append(float(content[i-1].split()[1]))\n",
    "        file.close()\n",
    "        weights0=np.array(weights0)\n",
    "        weights1=np.array(weights1)\n",
    "        weights0=weights0[negative_indices]\n",
    "        weights1=weights1[positive_indices]\n",
    "        weights_full=np.concatenate((weights0,weights1))\n",
    "        weights_full=weights_full[ind[0:]]\n",
    "    else:\n",
    "        weights_full=np.ones(len(idxs_full))\n",
    "    return(xtr_batch, ytr_batch, data_batch, weights_full, idxs_full)\n",
    "\n",
    "def perf_measure(y_actual, y_hat, cutoff=0.5):\n",
    "    TP = 0\n",
    "    FP = 0\n",
    "    TN = 0\n",
    "    FN = 0\n",
    "\n",
    "    for i in range(len(y_hat)): \n",
    "        if y_actual[i]==1 and y_hat[i]>cutoff:\n",
    "            TP += 1\n",
    "        if y_hat[i]>cutoff and y_actual[i]==0:\n",
    "            FP += 1\n",
    "        if y_actual[i]==0 and y_hat[i]<cutoff:\n",
    "            TN += 1\n",
    "        if y_hat[i]<cutoff and y_actual[i]==1:\n",
    "            FN += 1\n",
    "\n",
    "    return(TP, FP, TN, FN)\n",
    "\n",
    "def SemiTransform(Xtrain,Ytrain,probtrain):\n",
    "    X1=Xtrain[np.where(Ytrain==1)[0]]\n",
    "    X0 = Xtrain[np.where(Ytrain==0)[0]]\n",
    "    Xsemi=np.row_stack((X1,X0,X0))\n",
    "    prob0 = probtrain[np.where(Ytrain==0)[0]]\n",
    "    Y1=Ytrain[np.where(Ytrain==1)[0]]\n",
    "    Y0 = Ytrain[np.where(Ytrain==0)[0]]\n",
    "    Ysemi=np.concatenate((Y1,Y0,Y0+1))\n",
    "\n",
    "    # c=np.mean(probtrain[Ytrain == 1][:,1])\n",
    "    c=np.max(probtrain[np.where(Ytrain==1)[0]])\n",
    "    p=prob0\n",
    "    w=p/(1-p)\n",
    "    w*=(1-c)/c\n",
    "    weights = np.ones(len(Ysemi))\n",
    "    weights[len(Y1):len(Y1)+len(Y0)] = 1-w\n",
    "    weights[len(Y1) + len(Y0):] = w\n",
    "    Ysemi = np.column_stack((Ysemi,np.abs(Ysemi-1)))\n",
    "\n",
    "    return Xsemi, Ysemi, weights\n",
    "\n",
    "#hyperparameters\n",
    "element_names_array=['H','He','Li','Be','B','C','N','O','F','Ne','Na','Mg','Al','Si','P','S','Cl', 'Ar','K','Ca','Sc','Ti','V','Cr','Mn','Fe','Co','Ni','Cu','Zn','Ga','Ge','As','Se','Br','Kr','Rb','Sr','Y','Zr','Nb','Mo','Tc','Ru','Rh','Pd','Ag','Cd','In','Sn','Sb','Te','I','Xe','Cs','Ba','La','Ce','Pr','Nd','Pm','Sm','Eu','Gd','Tb','Dy','Ho','Er','Tm','Yb','Lu','Hf','Ta','W','Re','Os','Ir','Pt','Au','Hg','Tl','Pb','Bi','Po','At','Rn','Fr','Ra','Ac','Th','Pa','U','Np','Pu']\n",
    "\n",
    "np.random.seed()\n",
    "random.seed()\n",
    "M = 30\n",
    "tf.compat.v1.disable_eager_execution()\n",
    "randint=np.random.randint(0,5,5)\n",
    "tstep=[1e-2, 5e-3, 2e-3, 5e-4, 2e-4][randint[0]]\n",
    "num_steps=800000\n",
    "no_h1 = [30,40,50,60,80][randint[1]]\n",
    "no_h2= [30,40,50,60,80][randint[2]]\n",
    "batch_size = [512,512,1024,1024,1024][randint[3]]\n",
    "semi_starting=[20000,40000,60000,80000,100000][randint[4]]\n",
    "\n",
    "neg_pos_ratio=20\n",
    "weight_for_0 = (1 + neg_pos_ratio) / (2*neg_pos_ratio)\n",
    "weight_for_1 = (1 + neg_pos_ratio) / (2*1)\n",
    "\n",
    "model_name_params=str(randint[0]) + str(randint[1]) + str(randint[2]) + str(randint[3]) + str(randint[4])\n",
    "\n",
    "xtr,ytr,batch_data,weights,idxs=get_batch(batch_size, neg_pos_ratio, use_semi_weights=False, model_name=model_name_params)\n",
    "\n",
    "x = tf.compat.v1.placeholder(tf.float32, shape=[None, xtr.shape[1]])\n",
    "y_ = tf.compat.v1.placeholder(tf.float32, shape=[None, 2])\n",
    "\n",
    "W1 = tf.Variable(tf.random.truncated_normal([xtr.shape[1],M],0,3)) #shape, mean, std\n",
    "F1 = tf.Variable(tf.random.truncated_normal([M,no_h1],0,1))\n",
    "F2 = tf.Variable(tf.random.truncated_normal([no_h1,no_h2],0,1))\n",
    "F3 = tf.Variable(tf.random.truncated_normal([no_h2,2],0,1))\n",
    "b1 = tf.Variable(tf.random.truncated_normal([no_h1],0,1))\n",
    "b2 = tf.Variable(tf.random.truncated_normal([no_h2],0,1))\n",
    "b3 = tf.Variable(tf.random.truncated_normal([2],0,1))\n",
    "\n",
    "semi_weights=tf.compat.v1.placeholder(tf.float32, shape=[None,1])\n",
    "\n",
    "sess = tf.compat.v1.InteractiveSession()\n",
    "sess.run(tf.compat.v1.initialize_all_variables())\n",
    "\n",
    "z0_raw = tf.multiply(tf.expand_dims(x,2),tf.expand_dims(W1,0)) #(ntr, I, M)\n",
    "tempmean,var = tf.nn.moments(z0_raw,axes=[1])\n",
    "z0 = tf.concat([tf.reduce_sum(z0_raw,1)],1) #(ntr, M)\n",
    "z1 = tf.add(tf.matmul(z0,F1),b1) #(ntr, no_h1)\n",
    "a1 = tf.tanh(z1) #(ntr, no_h1)\n",
    "z2= tf.add(tf.matmul(a1,F2),b2) #(ntr,no_h1)\n",
    "a2= tf.tanh(z2) #(ntr, no_h1)\n",
    "z3 = tf.add(tf.matmul(a2,F3),b3) #(ntr, 2)\n",
    "a3 = tf.nn.softmax(z3) #(ntr, 2)\n",
    "clipped_y = tf.clip_by_value(a3, 1e-10, 1.0)\n",
    "cross_entropy = -tf.reduce_sum(tf.multiply(y_*tf.math.log(clipped_y)*np.array([weight_for_1,weight_for_0]),semi_weights))\n",
    "correct_prediction = tf.equal(tf.argmax(a3,1), tf.argmax(y_,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "performance_array=[]\n",
    "loss_array=[]\n",
    "element_sum=np.zeros((94,1))\n",
    "epoch_counter=0\n",
    "xtr_new=xtr\n",
    "ytr_new=ytr\n",
    "\n",
    "train_step = tf.compat.v1.train.AdamOptimizer(tstep).minimize(cross_entropy)\n",
    "sess.run(tf.compat.v1.initialize_all_variables())\n",
    "full_weights=np.ones((len(ytr),1))\n",
    "best_perf=0\n",
    "xval,yval,val_data=get_batch_val(neg_pos_ratio)\n",
    "\n",
    "W1_val=[]\n",
    "F1_val=[]\n",
    "F2_val=[]\n",
    "F3_val=[]\n",
    "b1_val=[]\n",
    "b2_val=[]\n",
    "b3_val=[]\n",
    "print('Model Initial Training')\n",
    "\n",
    "#change to semi_starting\n",
    "for i in range(100):  \n",
    "    epoch_counter=epoch_counter+1           \n",
    "    batchx,batchy,batch_data,weights,idxs=get_batch(batch_size, neg_pos_ratio, use_semi_weights=False, model_name=model_name_params)   \n",
    "    indB = list(range(0,len(xtr_new)))\n",
    "    random.shuffle(indB)\n",
    "    current_weights=full_weights[indB[0:batch_size],:] \n",
    "    train_step.run(feed_dict={x: batchx, y_: batchy, semi_weights: current_weights})\n",
    "    if(i%1000==0):  \n",
    "        preds=a3.eval(feed_dict={x: xval, y_: yval, semi_weights: full_weights})\n",
    "        TP, FP, TN, FN=perf_measure(np.array(yval)[:,0],np.array(preds)[:,0])\n",
    "        val_accuracy=accuracy.eval(feed_dict={x: xval, y_: yval, semi_weights: current_weights})\n",
    "        train_accuracy=accuracy.eval(feed_dict={x: batchx, y_: batchy, semi_weights: current_weights})\n",
    "        performance_array.append([train_accuracy,val_accuracy, TP, FP, TN, FN])\n",
    "        print(i)\n",
    "        print([train_accuracy,val_accuracy, TP, FP, TN, FN])\n",
    "        #np.savetxt('performance_matrix_TL_v3_' + str(randint[0]) + str(randint[1]) + str(randint[2]) + str(randint[3]) + '.txt',performance_array, fmt='%s')\n",
    "        if(val_accuracy>best_perf):\n",
    "            best_perf=val_accuracy\n",
    "            W1_val=sess.run(W1)\n",
    "            F1_val=sess.run(F1)\n",
    "            F2_val=sess.run(F2)\n",
    "            F3_val=sess.run(F3)\n",
    "            b1_val=sess.run(b1)\n",
    "            b2_val=sess.run(b2)\n",
    "            b3_val=sess.run(b3)\n",
    "    \n",
    "#print out all preds to a file (for weighting for semi-supervised learning)\n",
    "file_output = open(\"semi_weights_testing_pos_30M\" + model_name_params +  \".txt\",\"a\")\n",
    "file_positives=open('icsd_full_data_unique_no_frac_no_penta_2020.txt', 'r')\n",
    "Lines = file_positives.readlines()\n",
    "for line in Lines:\n",
    "    xtr=get_features([line.replace('\\n','')])\n",
    "    ytr=[[0,1]]\n",
    "    pred=a3.eval(feed_dict={x: xtr, y_: ytr, semi_weights: current_weights, W1:W1_val, F1:F1_val, F2:F2_val, F3:F3_val, b1:b1_val, b2:b2_val, b3:b3_val })\n",
    "    file_output.write(line.replace('\\n','') +  ' ' + str(pred[0][0]) + '\\n')   \n",
    "file_positives.close()\n",
    "file_output.close()\n",
    "file_output = open(\"semi_weights_testing_neg_30M\" + model_name_params +  \".txt\",\"a\")\n",
    "file_negatives=open('standard_neg_ex_tr_val_v5_balanced_shuffled.txt','r')\n",
    "Lines = file_negatives.readlines()\n",
    "for line in Lines:\n",
    "    xtr=get_features([line.replace('\\n','')])\n",
    "    ytr=[[0,1]]\n",
    "    pred=a3.eval(feed_dict={x: xtr, y_: ytr, semi_weights: current_weights, W1:W1_val, F1:F1_val, F2:F2_val, F3:F3_val, b1:b1_val, b2:b2_val, b3:b3_val })\n",
    "    file_output.write(line.replace('\\n','') +  ' ' + str(pred[0][0])+ '\\n') \n",
    "file_negatives.close()\n",
    "file_output.close()\n",
    "sess.close()\n",
    "\n",
    "print('Doing Semi-supervised Learning')\n",
    "np.random.seed()\n",
    "random.seed()\n",
    "M = 30\n",
    "x = tf.compat.v1.placeholder(tf.float32, shape=[None, xtr.shape[1]])\n",
    "y_ = tf.compat.v1.placeholder(tf.float32, shape=[None, 2])\n",
    "\n",
    "W1 = tf.Variable(tf.random.truncated_normal([xtr.shape[1],M],0,3)) #shape, mean, std\n",
    "F1 = tf.Variable(tf.random.truncated_normal([M,no_h1],0,1))\n",
    "F2 = tf.Variable(tf.random.truncated_normal([no_h1,no_h2],0,1))\n",
    "F3 = tf.Variable(tf.random.truncated_normal([no_h2,2],0,1))\n",
    "b1 = tf.Variable(tf.random.truncated_normal([no_h1],0,1))\n",
    "b2 = tf.Variable(tf.random.truncated_normal([no_h2],0,1))\n",
    "b3 = tf.Variable(tf.random.truncated_normal([2],0,1))\n",
    "semi_weights=tf.compat.v1.placeholder(tf.float32, shape=[None,1])\n",
    "\n",
    "sess = tf.compat.v1.InteractiveSession()\n",
    "sess.run(tf.compat.v1.initialize_all_variables())\n",
    "\n",
    "z0_raw = tf.multiply(tf.expand_dims(x,2),tf.expand_dims(W1,0)) #(ntr, I, M)\n",
    "tempmean,var = tf.nn.moments(z0_raw,axes=[1])\n",
    "z0 = tf.concat([tf.reduce_sum(z0_raw,1)],1) #(ntr, M)\n",
    "z1 = tf.add(tf.matmul(z0,F1),b1) #(ntr, no_h1)\n",
    "a1 = tf.tanh(z1) #(ntr, no_h1)\n",
    "z2= tf.add(tf.matmul(a1,F2),b2) #(ntr,no_h1)\n",
    "a2= tf.tanh(z2) #(ntr, no_h1)\n",
    "z3 = tf.add(tf.matmul(a2,F3),b3) #(ntr, 2)\n",
    "a3 = tf.nn.softmax(z3) #(ntr, 2)\n",
    "\n",
    "clipped_y = tf.clip_by_value(a3, 1e-10, 1.0)\n",
    "cross_entropy = -tf.reduce_sum(tf.multiply(y_*tf.math.log(clipped_y),semi_weights))\n",
    "correct_prediction = tf.equal(tf.argmax(a3,1), tf.argmax(y_,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "performance_array=[]\n",
    "loss_array=[]\n",
    "element_sum=np.zeros((94,1))\n",
    "epoch_counter=0\n",
    "best_perf=0\n",
    "train_step = tf.compat.v1.train.AdamOptimizer(tstep).minimize(cross_entropy)\n",
    "sess.run(tf.compat.v1.initialize_all_variables())\n",
    "\n",
    "#change to num_steps\n",
    "for i in range(1):   \n",
    "    epoch_counter=epoch_counter+1           \n",
    "    batchx,batchy,batch_data,weights,idxs=get_batch(batch_size, neg_pos_ratio, use_semi_weights=True, model_name=model_name_params)\n",
    "    weights=np.reshape(weights, [len(weights),1])        \n",
    "    train_step.run(feed_dict={x: batchx, y_: batchy, semi_weights: weights})\n",
    "    #loss_array.append([])\n",
    "    if(i%1000==0):\n",
    "        preds=a3.eval(feed_dict={x: xval, y_: yval, semi_weights: weights})\n",
    "        TP, FP, TN, FN=perf_measure(np.array(yval)[:,0],np.array(preds)[:,0])\n",
    "        val_accuracy=accuracy.eval(feed_dict={x: xval, y_: yval, semi_weights: weights})\n",
    "        train_accuracy=accuracy.eval(feed_dict={x: batchx, y_: batchy, semi_weights: weights})\n",
    "        performance_array.append([train_accuracy,val_accuracy, TP, FP, TN, FN])\n",
    "        print([train_accuracy,val_accuracy, TP, FP, TN, FN])\n",
    "        np.savetxt('performance_matrix_TL_v3_30M_' + model_name_params+ '.txt',performance_array, fmt='%s')\n",
    "\n",
    "    if(i%1000==0):\n",
    "        if(val_accuracy>best_perf):\n",
    "            model_name='30M_synth_v3_semi' + model_name_params + '.txt'\n",
    "            best_perf=val_accuracy\n",
    "            W1_val=sess.run(W1)\n",
    "            F1_val=sess.run(F1)\n",
    "            F2_val=sess.run(F2)\n",
    "            F3_val=sess.run(F3)\n",
    "            b1_val=sess.run(b1)\n",
    "            b2_val=sess.run(b2)\n",
    "            b3_val=sess.run(b3)\n",
    "            np.savetxt('W1_' + model_name, W1_val)\n",
    "            np.savetxt('F1_' + model_name, F1_val)\n",
    "            np.savetxt('F2_' + model_name, F2_val)\n",
    "            np.savetxt('F3_' + model_name, F3_val)\n",
    "            np.savetxt('b1_' + model_name, b1_val)\n",
    "            np.savetxt('b2_' + model_name, b2_val)\n",
    "            np.savetxt('b3_' + model_name, b3_val)\n",
    "sess.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
