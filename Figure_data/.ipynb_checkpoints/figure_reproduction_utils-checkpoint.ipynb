{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d34aa571",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "IPython.notebook.set_autosave_interval(60000)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autosaving every 60 seconds\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from numpy import trapz\n",
    "import matplotlib.pyplot as plt\n",
    "from io import StringIO\n",
    "import re\n",
    "import pymatgen as mg\n",
    "import tensorflow as tf\n",
    "import random\n",
    "import os\n",
    "from pymatgen.core.periodic_table import Element\n",
    "import json\n",
    "import linecache\n",
    "from pymatgen.core import Composition\n",
    "import scipy\n",
    "from scipy.stats import *\n",
    "%autosave 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a36edeaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def balanced_frac(materials, ox_dict):\n",
    "    balanced = []\n",
    "    for material in materials:\n",
    "        try:\n",
    "            ox_guesses = list(Composition(material).oxi_state_guesses(oxi_states_override=ox_dict))\n",
    "            if (len(ox_guesses) > 0):\n",
    "                for j in range(len(ox_guesses)):\n",
    "                    success=0\n",
    "                    for key in (ox_guesses[j]):\n",
    "                        coeff=(ox_guesses[j][key])\n",
    "                        if(coeff.is_integer()):\n",
    "                            success=success+1\n",
    "                        else:\n",
    "                            continue\n",
    "                    if(success==len(ox_guesses[j])):\n",
    "                        #print(ox_guesses[j])\n",
    "                        return(['True', ox_guesses[j]])\n",
    "            if(len(Composition(material))==1):\n",
    "                return(['True','0'])\n",
    "            else:\n",
    "                balanced.append(0)\n",
    "            #print(str(material), len(material))\n",
    "        except:\n",
    "            balanced.append(0)\n",
    "    return(['False', 'None'])\n",
    "\n",
    "def balanced_frac2(materials, ox_dict):\n",
    "    balanced=[]\n",
    "    for material in materials:\n",
    "        try:\n",
    "            ox_guesses = list(Composition(material).oxi_state_guesses(oxi_states_override=common_ox_dict))\n",
    "            if (len(ox_guesses) > 0):\n",
    "                for j in range(len(ox_guesses)):\n",
    "                    success=0\n",
    "                    for key in (ox_guesses[j]):\n",
    "                        coeff=(ox_guesses[j][key])\n",
    "                        if(coeff.is_integer()):\n",
    "                            success=success+1\n",
    "                        else:\n",
    "                            continue\n",
    "                    if(success==len(ox_guesses[j])):\n",
    "                        #print(ox_guesses[j])\n",
    "                        balanced.append([material,'True', ox_guesses[j]])\n",
    "                    else:\n",
    "                        balanced.append([material,'False','0'])\n",
    "            elif(len(Composition(material))==1):\n",
    "                balanced.append([material,'True','0'])\n",
    "            else:\n",
    "                balanced.append([material,'False','0'])\n",
    "        except:\n",
    "            balanced.append([material,'False','0'])\n",
    "    return(balanced)\n",
    "\n",
    "def get_weighted_F1(TP,FP,TN,FN,num_positive,num_negative):\n",
    "    num_positive_class=num_positive/(num_positive+num_negative)\n",
    "    num_negative_class=num_negative/(num_positive+num_negative)\n",
    "    pos_prec=TP/(TP+FP)\n",
    "    pos_rec=TP/(TP+FN)\n",
    "    neg_prec=TN/(TN+FN)\n",
    "    neg_rec=TN/(TN+FP)\n",
    "    F1_pos=2*(pos_prec*pos_rec)/(pos_prec+pos_rec)\n",
    "    F1_neg=2*(neg_prec*neg_rec)/(neg_prec+neg_rec)\n",
    "    F1_weighted=(num_positive_class*F1_pos) + (num_negative_class*F1_neg)\n",
    "    return(F1_weighted)\n",
    "\n",
    "def get_batch(batch_size, neg_positive_ratio, use_semi_weights, model_name, seed=False, seed_value=0):\n",
    "    def random_lines(filename, file_size, num_samples):\n",
    "        idxs = random.sample(range(1,file_size), num_samples)\n",
    "        return ([linecache.getline(filename, i) for i in idxs], idxs)\n",
    "    if(seed):\n",
    "        random.seed(seed_value)\n",
    "        np.random.seed(seed_value)\n",
    "    else:\n",
    "        random.seed()\n",
    "        np.random.seed()\n",
    "    num_positive_examples=int(np.floor(batch_size*(1/(1+neg_positive_ratio))))\n",
    "    num_negative_examples=batch_size-num_positive_examples\n",
    "    noTr_positives=48200 #number positive examples in train set\n",
    "    noTr_negatives=noTr_positives*neg_positive_ratio #no. negatives examples in train set\n",
    "    noTr = noTr_positives + (noTr_negatives) #total size of train set\n",
    "    #only sample from first 90% of dataset; shuffle first\n",
    "    data1=[]\n",
    "    pulled_lines1,idxs1=random_lines('icsd_full_data_unique_no_frac_no_penta_2020.txt', noTr_positives,num_positive_examples)\n",
    "    for line in pulled_lines1:\n",
    "        data1.append(line.replace('\\n',''))\n",
    "    data0=[]\n",
    "    pulled_lines0,idxs0=random_lines('standard_neg_ex_tr_val_v5_balanced_shuffled.txt', noTr_negatives, num_negative_examples)\n",
    "    for line in pulled_lines0:\n",
    "        data0.append(line.replace('\\n',''))\n",
    "    #do consistent shuffling once examples have been chosen\n",
    "    random.seed(3)\n",
    "    np.random.seed(3)\n",
    "    #shuffle the positive and negative examples with themselves\n",
    "    negative_indices=list(range(0,len(data0)))\n",
    "    random.shuffle(negative_indices)\n",
    "    positive_indices=list(range(0,len(data1)))\n",
    "    random.shuffle(positive_indices)\n",
    "    data0=np.array(data0)\n",
    "    data1=np.array(data1)\n",
    "    idxs0=np.array(idxs0)\n",
    "    idxs1=np.array(idxs1)\n",
    "    data0=data0[negative_indices]\n",
    "    data1=data1[positive_indices]\n",
    "    featmat0=get_features(data0)\n",
    "    featmat1=get_features(data1)\n",
    "    idxs0=idxs0[negative_indices]\n",
    "    idxs1=idxs1[positive_indices]\n",
    "    #get labels\n",
    "    labs = np.zeros((len(data0) + len(data1),1))\n",
    "    for ind,ent in enumerate(data1):\n",
    "        labs[ind,0] = 1\n",
    "    unique, counts = np.unique(labs, return_counts=True)\n",
    "    ind0 = np.where(labs==0)[0] #indices of label=0\n",
    "    ind1 = np.where(labs==1)[0] #indices of label=1\n",
    "    #combine positives and negatives and shuffle\n",
    "    featmat3 = np.concatenate((featmat0,featmat1)) #set legths of labels 0 and 1 to be the same in the new feature matrix featmat3\n",
    "    datasorted = np.concatenate((data0,data1)) #data ordered the same as featmat3\n",
    "    labs3 = np.concatenate((labs[ind0], labs[ind1]), axis=0) #labels ordered the same as featmat3\n",
    "    idxs_full=np.concatenate((idxs0,idxs1))\n",
    "    noS = len(featmat3)\n",
    "    ind = list(range(0,noS)) #training set index\n",
    "    random.shuffle(ind) #shuffle training set index\n",
    "    #indB = list(range(0,noTr_subset)) #used later for batch\n",
    "    labs3 = np.column_stack((labs3,np.abs(labs3-1)))\n",
    "    xtr_batch = featmat3[ind[0:],:]\n",
    "    ytr_batch = labs3[ind[0:],:]\n",
    "    data_batch=datasorted[ind[0:]]\n",
    "    idxs_full=idxs_full[ind[0:]]\n",
    "    #all weights stuff here\n",
    "    weights_full=[]\n",
    "    if(use_semi_weights):\n",
    "        weights1=[]\n",
    "        file=open('semi_weights_testing_pos_20M' + model_name + '.txt','r')\n",
    "        content=file.readlines()\n",
    "        weights1=[]\n",
    "        for i in idxs1:\n",
    "            weights1.append(float(content[i-1].split()[1]))\n",
    "        file.close()\n",
    "        weights0=[]\n",
    "        file=open('semi_weights_testing_neg_20M' + model_name + '.txt','r')\n",
    "        content=file.readlines()\n",
    "        for i in idxs0:\n",
    "            weights0.append(float(content[i-1].split()[1]))\n",
    "        file.close()\n",
    "        weights0=np.array(weights0)\n",
    "        weights1=np.array(weights1)\n",
    "        weights0=weights0[negative_indices]\n",
    "        weights1=weights1[positive_indices]\n",
    "        weights_full=np.concatenate((weights0,weights1))\n",
    "        weights_full=weights_full[ind[0:]]\n",
    "    else:\n",
    "        weights_full=np.ones(len(idxs_full))\n",
    "    return(xtr_batch, ytr_batch, data_batch, weights_full, idxs_full)\n",
    "\n",
    "def get_batch_val(neg_positive_ratio):\n",
    "    random.seed(3)\n",
    "    np.random.seed(3)\n",
    "    noTr_positives=48200 #number positive examples in train set\n",
    "    noTr_negatives_start=noTr_positives*neg_positive_ratio\n",
    "    noTr_negatives=noTr_positives*neg_positive_ratio #no. negatives examples in train set\n",
    "    noTr = noTr_positives + (noTr_negatives) #total size of train set\n",
    "    #only sample from first 90% of dataset; shuffle first\n",
    "    data1=[]\n",
    "    f=open('icsd_full_data_unique_no_frac_no_penta_2020.txt')\n",
    "    i=0\n",
    "    for line in f:\n",
    "        if(i>noTr_positives and i<noTr_positives*1.05):\n",
    "            data1.append(line.replace('\\n',''))\n",
    "        i+=1\n",
    "    f.close()\n",
    "    data0=[]\n",
    "    f=open('standard_neg_ex_tr_val_v5_balanced_shuffled.txt')\n",
    "    i=0\n",
    "    for line in f:\n",
    "        if(i>noTr_negatives_start and i<noTr_negatives_start + (noTr_negatives*0.05)):\n",
    "            data0.append(line.replace('\\n',''))\n",
    "        i+=1\n",
    "    f.close() \n",
    "    #shuffle the positive and negative examples with themselves\n",
    "    negative_indices=list(range(0,len(data0)))\n",
    "    random.shuffle(negative_indices)\n",
    "    positive_indices=list(range(0,len(data1)))\n",
    "    random.shuffle(positive_indices)\n",
    "    data0=np.array(data0)\n",
    "    data1=np.array(data1)\n",
    "    data0=data0[negative_indices]\n",
    "    data1=data1[positive_indices]\n",
    "    featmat0=get_features(data0)\n",
    "    featmat1=get_features(data1)\n",
    "    #get labels\n",
    "    labs = np.zeros((len(data0) + len(data1),1))\n",
    "    for ind,ent in enumerate(data1):\n",
    "        labs[ind,0] = 1\n",
    "    unique, counts = np.unique(labs, return_counts=True)\n",
    "    ind0 = np.where(labs==0)[0] #indices of label=0\n",
    "    ind1 = np.where(labs==1)[0] #indices of label=1\n",
    "    #print(len(ind0),len(ind1))\n",
    "    #combine positives and negatives and shuffle\n",
    "    featmat3 = np.concatenate((featmat0,featmat1)) #set legths of labels 0 and 1 to be the same in the new feature matrix featmat3\n",
    "    datasorted = np.concatenate((data0,data1)) #data ordered the same as featmat3\n",
    "    labs3 = np.concatenate((labs[ind0], labs[ind1]), axis=0) #labels ordered the same as featmat3\n",
    "    noS = len(featmat3)\n",
    "    ind = list(range(0,noS)) #training set index\n",
    "    random.shuffle(ind) #shuffle training set index\n",
    "    labs3 = np.column_stack((labs3,np.abs(labs3-1)))\n",
    "    xtr_batch = featmat3[ind[0:],:]\n",
    "    ytr_batch = labs3[ind[0:],:]\n",
    "    data_batch=datasorted[ind[0:]]\n",
    "    return(xtr_batch, ytr_batch, data_batch)\n",
    "\n",
    "def get_recall(preds,actual_values,precision=0.50):\n",
    "    difference=100 #variable for how close the precision is to the desired value\n",
    "    for cutoff_value in np.linspace(1,0,100):\n",
    "        TP, FP, TN, FN=perf_measure(np.array(yte)[:,0],np.array(preds)[:,0], cutoff=cutoff_value)\n",
    "        if(TP>0):\n",
    "            cutoff_precision=TP/(TP+FP)\n",
    "            difference=cutoff_precision-precision\n",
    "            if(difference<0):\n",
    "                return(TP/(TP+FN), cutoff_value)\n",
    "\n",
    "def get_batch_test(neg_positive_ratio, seed=3):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    noTr_positives=48200 #number positive examples in train set\n",
    "    noTr_negatives_start=noTr_positives*neg_positive_ratio\n",
    "    noTr_negatives=noTr_positives*neg_positive_ratio #no. negatives examples in train set\n",
    "    noTr = noTr_positives + (noTr_negatives) #total size of train set\n",
    "    #only sample from first 90% of dataset ; shuffle first\n",
    "    data1=[]\n",
    "    f=open('icsd_full_data_unique_no_frac_no_penta_2020.txt')\n",
    "    i=0\n",
    "    for line in f:\n",
    "        if(i>noTr_positives*1.05 and i<noTr_positives*1.10):\n",
    "            data1.append(line.replace('\\n',''))\n",
    "        i+=1\n",
    "    f.close()\n",
    "    data0=[]\n",
    "    f=open('standard_neg_ex_tr_val_v5_balanced_shuffled.txt')\n",
    "    i=0\n",
    "    for line in f:\n",
    "        if(i>noTr_negatives_start*1.05 and i<noTr_negatives_start*1.05 + (noTr_negatives*0.05)):\n",
    "            data0.append(line.replace('\\n',''))\n",
    "        i+=1\n",
    "    f.close()\n",
    "    #shuffle the positive and negative examples with themselves\n",
    "    negative_indices=list(range(0,len(data0)))\n",
    "    random.shuffle(negative_indices)\n",
    "    positive_indices=list(range(0,len(data1)))\n",
    "    random.shuffle(positive_indices)\n",
    "    data0=np.array(data0)\n",
    "    data1=np.array(data1)\n",
    "    data0=data0[negative_indices]\n",
    "    data1=data1[positive_indices]\n",
    "    featmat0=get_features(data0)\n",
    "    featmat1=get_features(data1)\n",
    "    #get labels\n",
    "    labs = np.zeros((len(data0) + len(data1),1))\n",
    "    for ind,ent in enumerate(data1):\n",
    "        labs[ind,0] = 1\n",
    "    unique, counts = np.unique(labs, return_counts=True)\n",
    "    ind0 = np.where(labs==0)[0] #indices of label=0\n",
    "    ind1 = np.where(labs==1)[0] #indices of label=1\n",
    "    #print(len(ind0),len(ind1))\n",
    "    #combine positives and negatives and shuffle\n",
    "    featmat3 = np.concatenate((featmat0,featmat1)) #set legths of labels 0 and 1 to be the same in the new feature matrix featmat3\n",
    "    datasorted = np.concatenate((data0,data1)) #data ordered the same as featmat3\n",
    "    labs3 = np.concatenate((labs[ind0], labs[ind1]), axis=0) #labels ordered the same as featmat3\n",
    "    noS = len(featmat3)\n",
    "    ind = list(range(0,noS)) #training set index\n",
    "    random.shuffle(ind) #shuffle training set index\n",
    "    #indB = list(range(0,noTr_subset)) #used later for batch\n",
    "    labs3 = np.column_stack((labs3,np.abs(labs3-1)))\n",
    "    xtr_batch = featmat3[ind[0:],:]\n",
    "    ytr_batch = labs3[ind[0:],:]\n",
    "    data_batch=datasorted[ind[0:]]\n",
    "    return(xtr_batch, ytr_batch, data_batch)\n",
    "\n",
    "neg_pos_ratio=25\n",
    "weight_for_0 = (1 + neg_pos_ratio) / (2*neg_pos_ratio)\n",
    "weight_for_1 = (1 + neg_pos_ratio) / (2*1)\n",
    "def perf_measure(y_actual, y_hat, cutoff=0.5):\n",
    "    TP = 0\n",
    "    FP = 0\n",
    "    TN = 0\n",
    "    FN = 0\n",
    "\n",
    "    for i in range(len(y_hat)):\n",
    "        if y_actual[i]==1 and y_hat[i]>cutoff:\n",
    "            TP += 1\n",
    "        if y_hat[i]>cutoff and y_actual[i]==0:\n",
    "            FP += 1\n",
    "        if y_actual[i]==0 and y_hat[i]<cutoff:\n",
    "            TN += 1\n",
    "        if y_hat[i]<cutoff and y_actual[i]==1:\n",
    "            FN += 1\n",
    "\n",
    "    return(TP, FP, TN, FN)\n",
    "\n",
    "#charge balancing over time\n",
    "#full_ox_dict = {'H': [-1, 0, 1], 'He': [0], 'Li': [0, 1], 'Be': [0, 1, 2], 'B': [-5, -1, 0, 1, 2, 3], 'C': [-4, -3, -2, -1, 0, 1, 2, 3, 4], 'N': [-3, -2, -1, 0, 1, 2, 3, 4, 5], 'O': [-2, -1, 0, 1, 2], 'F': [-1, 0], 'Ne': [0], 'Na': [-1, 0, 1], 'Mg': [0, 1, 2], 'Al': [-2, -1, 0, 1, 2, 3], 'Si': [-4, -3, -2, -1, 0, 1, 2, 3, 4], 'P': [-3, -2, -1, 0, 1, 2, 3, 4, 5], 'S': [-2, -1, 0, 1, 2, 3, 4, 5, 6], 'Cl': [-1, 0, 1, 2, 3, 4, 5, 6, 7], 'Ar': [0], 'K': [-1, 0, 1], 'Ca': [0, 1, 2], 'Sc': [0, 1, 2, 3], 'Ti': [-2, -1, 0, 1, 2, 3, 4], 'V': [-3, -1, 0, 1, 2, 3, 4, 5], 'Cr': [-4, -2, -1, 0, 1, 2, 3, 4, 5, 6], 'Mn': [-3, -2, -1, 0, 1, 2, 3, 4, 5, 6, 7], 'Fe': [-4, -2, -1, 0, 1, 2, 3, 4, 5, 6], 'Co': [-3, -1, 0, 1, 2, 3, 4, 5], 'Ni': [-2, -1, 0, 1, 2, 3, 4], 'Cu': [-2, 0, 1, 2, 3, 4], 'Zn': [-2, 0, 1, 2], 'Ga': [-5, -4, -2, -1, 0, 1, 2, 3], 'Ge': [-4, -3, -2, -1, 0, 1, 2, 3, 4], 'As': [-3, -2, -1, 0, 1, 2, 3, 4, 5], 'Se': [-2, -1, 0, 1, 2, 3, 4, 5, 6], 'Br': [-1, 0, 1, 3, 4, 5, 7], 'Kr': [0, 2], 'Rb': [-1, 0, 1], 'Sr': [0, 1, 2], 'Y': [0, 1, 2, 3], 'Zr': [-2, 0, 1, 2, 3, 4], 'Nb': [-3, -1, 0, 1, 2, 3, 4, 5], 'Mo': [-4, -2, -1, 0, 1, 2, 3, 4, 5, 6], 'Tc': [-3, -1, 0, 1, 2, 3, 4, 5, 6, 7], 'Ru': [-4, -2, 0, 1, 2, 3, 4, 5, 6, 7, 8], 'Rh': [-3, -1, 0, 1, 2, 3, 4, 5, 6], 'Pd': [0, 1, 2, 3, 4, 5, 6], 'Ag': [-2, -1, 0, 1, 2, 3, 4], 'Cd': [-2, 0, 1, 2], 'In': [-5, -2, -1, 0, 1, 2, 3], 'Sn': [-4, -3, -2, -1, 0, 1, 2, 3, 4], 'Sb': [-3, -2, -1, 0, 1, 2, 3, 4, 5], 'Te': [-2, -1, 0, 1, 2, 3, 4, 5, 6], 'I': [-1, 0, 1, 3, 4, 5, 6, 7], 'Xe': [0, 2, 4, 6, 8], 'Cs': [-1, 0, 1], 'Ba': [0, 1, 2], 'Hf': [-2, 0, 1, 2, 3, 4], 'Ta': [-3, -1, 0, 1, 2, 3, 4, 5], 'W': [-4, -2, -1, 0, 1, 2, 3, 4, 5, 6], 'Re': [-3, -1, 0, 1, 2, 3, 4, 5, 6, 7], 'Os': [-4, -2, -1, 0, 1, 2, 3, 4, 5, 6, 7, 8], 'Ir': [-3, -1, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9], 'Pt': [-3, -2, -1, 0, 1, 2, 3, 4, 5, 6], 'Au': [-3, -2, -1, 0, 1, 2, 3, 5], 'Hg': [-2, 0, 1, 2], 'Tl': [-5, -2, -1, 0, 1, 2, 3], 'Pb': [-4, -2, -1, 0, 1, 2, 3, 4], 'Bi': [-3, -2, -1, 0, 1, 2, 3, 4, 5], 'Po': [-2, 0, 2, 4, 5, 6], 'At': [-1, 0, 1, 3, 5, 7], 'Rn': [0, 2, 6], 'Fr': [0, 1], 'Ra': [0, 2], 'Rf': [0, 4], 'Db': [0, 5], 'Sg': [0, 6], 'Bh': [0, 7], 'Hs': [0, 8], 'Mt': [0], 'Ds': [0], 'Rg': [0], 'Cn': [0], 'Nh': [0], 'Fl': [0], 'Mc': [0], 'Lv': [0], 'Ts': [0], 'Og': [0], 'La': [0, 1, 2, 3], 'Ce': [0, 2, 3, 4], 'Pr': [0, 2, 3, 4], 'Nd': [0, 2, 3, 4], 'Pm': [0, 2, 3], 'Sm': [0, 2, 3], 'Eu': [0, 2, 3], 'Gd': [0, 1, 2, 3], 'Tb': [0, 1, 2, 3, 4], 'Dy': [0, 2, 3, 4], 'Ho': [0, 2, 3], 'Er': [0, 2, 3], 'Tm': [0, 2, 3], 'Yb': [0, 2, 3], 'Lu': [0, 2, 3], 'Ac': [0, 2, 3], 'Th': [0, 1, 2, 3, 4], 'Pa': [0, 2, 3, 4, 5], 'U': [0, 1, 2, 3, 4, 5, 6], 'Np': [0, 2, 3, 4, 5, 6, 7], 'Pu': [0, 1, 2, 3, 4, 5, 6, 7, 8], 'Am': [0, 2, 3, 4, 5, 6, 7, 8], 'Cm': [0, 2, 3, 4, 6], 'Bk': [0, 2, 3, 4], 'Cf': [0, 2, 3, 4], 'Es': [0, 2, 3, 4], 'Fm': [0, 2, 3], 'Md': [0, 2, 3], 'No': [0, 2, 3], 'Lr': [0, 3]}\n",
    "#common_ox_dict = {'H': [-1, 0, 1], 'He': [0], 'Li': [0, 1], 'Be': [0, 2], 'B': [0, 3], 'C': [-4, -3, -2, -1, 0, 1, 2, 3, 4], 'N': [-3, 0, 3, 5], 'O': [-2, 0], 'F': [-1, 0], 'Ne': [0], 'Na': [0, 1], 'Mg': [0, 2], 'Al': [0, 3], 'Si': [-4, 0, 4], 'P': [-3, 0, 3, 5], 'S': [-2, 0, 2, 4, 6], 'Cl': [-1, 0, 1, 3, 5, 7], 'Ar': [0], 'K': [0, 1], 'Ca': [0, 2], 'Sc': [0, 3], 'Ti': [0, 4], 'V': [0, 5], 'Cr': [0, 3, 6], 'Mn': [0, 2, 4, 7], 'Fe': [0, 2, 3, 6], 'Co': [0, 2, 3], 'Ni': [0, 2], 'Cu': [0, 2], 'Zn': [0, 2], 'Ga': [0, 3], 'Ge': [-4, 0, 2, 4], 'As': [-3, 0, 3, 5], 'Se': [-2, 0, 2, 4, 6], 'Br': [-1, 0, 1, 3, 5, 7], 'Kr': [0, 2], 'Rb': [0, 1], 'Sr': [0, 2], 'Y': [0, 3], 'Zr': [0, 4], 'Nb': [0, 5], 'Mo': [0, 4, 6], 'Tc': [0, 4, 7], 'Ru': [0, 2, 3, 4], 'Rh': [0, 3], 'Pd': [0, 2, 4], 'Ag': [0, 1], 'Cd': [0, 2], 'In': [0, 3], 'Sn': [-4, 0, 2, 4], 'Sb': [-3, 0, 3, 5], 'Te': [-2, 0, 2, 4, 6], 'I': [-1, 0, 1, 3, 5, 7], 'Xe': [0, 2, 4, 6], 'Cs': [0, 1], 'Ba': [0, 2], 'Hf': [0, 4], 'Ta': [0, 5], 'W': [0, 4, 6], 'Re': [0, 4], 'Os': [0, 4], 'Ir': [0, 3, 4], 'Pt': [0, 2, 4], 'Au': [0, 3], 'Hg': [0, 1, 2], 'Tl': [0, 1, 3], 'Pb': [0, 2, 4], 'Bi': [0, 3], 'Po': [-2, 0, 2, 4], 'At': [-1, 0, 1], 'Rn': [0, 2], 'Fr': [0, 1], 'Ra': [0, 2], 'Rf': [0, 4], 'Db': [0, 5], 'Sg': [0, 6], 'Bh': [0, 7], 'Hs': [0, 8], 'Mt': [0], 'Ds': [0], 'Rg': [0], 'Cn': [0], 'Nh': [0], 'Fl': [0], 'Mc': [0], 'Lv': [0], 'Ts': [0], 'Og': [0], 'La': [0, 3], 'Ce': [0, 3, 4], 'Pr': [0, 3], 'Nd': [0, 3], 'Pm': [0, 3], 'Sm': [0, 3], 'Eu': [0, 2, 3], 'Gd': [0, 3], 'Tb': [0, 3], 'Dy': [0, 3], 'Ho': [0, 3], 'Er': [0, 3], 'Tm': [0, 3], 'Yb': [0, 3], 'Lu': [0, 3], 'Ac': [0, 3], 'Th': [0, 4], 'Pa': [0, 5], 'U': [0, 4, 6], 'Np': [0, 5], 'Pu': [0, 4], 'Am': [0, 3], 'Cm': [0, 3], 'Bk': [0, 3], 'Cf': [0, 3], 'Es': [0, 3], 'Fm': [0, 3], 'Md': [0, 3], 'No': [0, 2], 'Lr': [0, 3]}\n",
    "mg_common_ox_dict = {}\n",
    "mg_full_ox_dict = {}\n",
    "species_in_use = ['Ac', 'Ag', 'Al', 'As', 'Au', 'B', 'Ba', 'Be', 'Bi', 'Br', 'C', 'Ca', 'Cd', 'Ce', 'Cl', 'Co', 'Cr', 'Cs', 'Cu', 'Dy', 'Er', 'Eu', 'F', 'Fe', 'Ga', 'Gd', 'Ge', 'H', 'Hf', 'Hg', 'Ho', 'I', 'In', 'Ir', 'K', 'La', 'Li', 'Lu', 'Mg', 'Mn', 'Mo', 'N', 'Na', 'Nb', 'Nd', 'Ni', 'Np', 'O', 'Os', 'P', 'Pa', 'Pb', 'Pd', 'Pr', 'Pt', 'Pu', 'Rb', 'Re', 'Rh', 'Ru', 'S', 'Sb', 'Sc', 'Se', 'Si', 'Sm', 'Sn', 'Sr', 'Ta', 'Tb', 'Te', 'Th', 'Ti', 'Tl', 'Tm', 'U', 'V', 'W', 'Y', 'Yb', 'Zn', 'Zr']\n",
    "for specie in species_in_use:\n",
    "    mg_common_ox_dict[specie] = list(mg.core.periodic_table.Specie(specie).common_oxidation_states)\n",
    "for specie in species_in_use:\n",
    "    mg_full_ox_dict[specie] = list(mg.core.periodic_table.Specie(specie).oxidation_states)\n",
    "full_ox_dict = mg_full_ox_dict\n",
    "common_ox_dict = mg_common_ox_dict\n",
    "full_ox_dict = {x: full_ox_dict[x] for x in full_ox_dict if x in species_in_use}\n",
    "common_ox_dict = {x: common_ox_dict[x] for x in common_ox_dict if x in species_in_use}\n",
    "element_names_array=['H','He','Li','Be','B','C','N','O','F','Ne','Na','Mg','Al','Si','P','S','Cl', 'Ar','K','Ca','Sc','Ti','V','Cr','Mn','Fe','Co','Ni','Cu','Zn','Ga','Ge','As','Se','Br','Kr','Rb','Sr','Y','Zr','Nb','Mo','Tc','Ru','Rh','Pd','Ag','Cd','In','Sn','Sb','Te','I','Xe','Cs','Ba','La','Ce','Pr','Nd','Pm','Sm','Eu','Gd','Tb','Dy','Ho','Er','Tm','Yb','Lu','Hf','Ta','W','Re','Os','Ir','Pt','Au','Hg','Tl','Pb','Bi','Po','At','Rn','Fr','Ra','Ac','Th','Pa','U','Np','Pu']\n",
    "\n",
    "def load_data(data, is_charge_balanced, max_atoms=5, max_coefficient=100000):\n",
    "    #takes input file (icsd_full_properties_no_frac_charges) and processes the data and applies some filters\n",
    "    output_array=[]\n",
    "    coeff_array=np.zeros((10000,1))\n",
    "    element_names_array=['H','He','Li','Be','B','C','N','O','F','Ne','Na','Mg','Al','Si','P','S','Cl', 'Ar','K','Ca','Sc','Ti','V','Cr','Mn','Fe','Co','Ni','Cu','Zn','Ga','Ge','As','Se','Br','Kr','Rb','Sr','Y','Zr','Nb','Mo','Tc','Ru','Rh','Pd','Ag','Cd','In','Sn','Sb','Te','I','Xe','Cs','Ba','La','Ce','Pr','Nd','Pm','Sm','Eu','Gd','Tb','Dy','Ho','Er','Tm','Yb','Lu','Hf','Ta','W','Re','Os','Ir','Pt','Au','Hg','Tl','Pb','Bi','Po','At','Rn','Fr','Ra','Ac','Th','Pa','U','Np','Pu']\n",
    "\n",
    "    for i in range(len(data)):\n",
    "        try:\n",
    "            comp=mg.core.composition.Composition(data[i,1])\n",
    "        except:\n",
    "            continue #bad formula\n",
    "\n",
    "        if(len(mg.core.composition.Composition(data[i,1]))==1):\n",
    "            continue\n",
    "\n",
    "        truth_array=[]\n",
    "        for element_name in mg.core.composition.Composition(data[i,1]).as_dict().keys():\n",
    "            if(element_name not in element_names_array):\n",
    "                truth_array.append('False')\n",
    "        if('False' in truth_array):\n",
    "            continue\n",
    "\n",
    "        if(is_charge_balanced):\n",
    "            if('True' in data[i][8]):\n",
    "                if(len(mg.core.composition.Composition(data[i,1]))<max_atoms):\n",
    "                    values=mg.core.composition.Composition(data[i,1]).as_dict().values()\n",
    "                    for value in values:\n",
    "                        coeff_array[int(value)]=coeff_array[int(value)]+1\n",
    "                    large_values=[x for x in values if x>max_coefficient]\n",
    "                    if(len(large_values)==0):\n",
    "                        output_array.append(mg.core.composition.Composition(data[i,1]).alphabetical_formula.replace(' ', ''))\n",
    "        else:\n",
    "            output_array.append(mg.core.composition.Composition(data[i,1]).alphabetical_formula.replace(' ', ''))\n",
    "    return(np.unique(output_array))\n",
    "\n",
    "def get_features(data0):\n",
    "    p = re.compile('[A-Z][a-z]?\\d*\\.?\\d*')\n",
    "    p3 = re.compile('[A-Z][a-z]?')\n",
    "    p5 = re.compile('\\d+\\.?\\d+|\\d+')\n",
    "    data0_ratio=[]\n",
    "    for i in data0:\n",
    "        x = i\n",
    "        p2 = p.findall(x)\n",
    "        temp1,temp2 = [], []\n",
    "        for x in p2:\n",
    "            temp1.append(Element[p3.findall(x)[0]].number)\n",
    "            kkk = p5.findall(x)\n",
    "            if len(kkk)<1:\n",
    "                temp2.append(1)\n",
    "            else:\n",
    "                temp2.append(kkk[0])\n",
    "        data0_ratio.append([temp1,list(map(float,temp2))])\n",
    "\n",
    "    I = 94\n",
    "    featmat0 = np.zeros((len(data0_ratio),I))\n",
    "    # featmat: n-hot vectors with fractions\n",
    "    for idx,ent in enumerate(data0_ratio):\n",
    "        for idy,at in enumerate(ent[0]):\n",
    "            featmat0[idx,at-1] = ent[1][idy]/sum(ent[1])\n",
    "    return(featmat0)\n",
    "\n",
    "\n",
    "def make_negative_data(num_examples, max_atoms=5, max_coefficient=11, seed=3, weighted=False):\n",
    "    output_array=[]\n",
    "    element_names_array=['H','He','Li','Be','B','C','N','O','F','Ne','Na','Mg','Al','Si','P','S','Cl', 'Ar','K','Ca','Sc','Ti','V','Cr','Mn','Fe','Co','Ni','Cu','Zn','Ga','Ge','As','Se','Br','Kr','Rb','Sr','Y','Zr','Nb','Mo','Tc','Ru','Rh','Pd','Ag','Cd','In','Sn','Sb','Te','I','Xe','Cs','Ba','La','Ce','Pr','Nd','Pm','Sm','Eu','Gd','Tb','Dy','Ho','Er','Tm','Yb','Lu','Hf','Ta','W','Re','Os','Ir','Pt','Au','Hg','Tl','Pb','Bi','Po','At','Rn','Fr','Ra','Ac','Th','Pa','U','Np','Pu']\n",
    "    element_sum=np.loadtxt('icsd_mpids_unique_element_proportions.txt')\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    while len(output_array) < num_examples:\n",
    "        if(len(output_array)%10000==0):\n",
    "            a=1\n",
    "        num_atoms=np.random.randint(2,max_atoms,1)[0] #number of atom types (binary, tern,  quat)\n",
    "        coeffs=np.random.randint(1,max_coefficient,num_atoms) #coeffs for each atom\n",
    "        if(weighted):\n",
    "            atomic_numbers=np.random.choice(94, num_atoms, p=np.reshape(element_sum, [94]))+1 #add one to give between 1,95\n",
    "        else:\n",
    "            atomic_numbers=np.random.randint(1,95,num_atoms)  #goes up to atomic number 94\n",
    "\n",
    "        output=''\n",
    "        for i in range(num_atoms):\n",
    "            output+=element_names_array[atomic_numbers[i]-1]\n",
    "            output+=str(coeffs[i])\n",
    "        if(mg.core.composition.Composition(output).alphabetical_formula.replace(' ', '') not in output_array):\n",
    "            output_array.append(mg.core.composition.Composition(output).alphabetical_formula.replace(' ', ''))\n",
    "    return(output_array)\n",
    "\n",
    "def get_coeffs(data):\n",
    "    coeffs = []\n",
    "    for entry in data:\n",
    "        coeffs.append(list(mg.core.Composition(entry).as_dict().values()))\n",
    "    return coeffs\n",
    "\n",
    "def get_max_coeff(coeffs):\n",
    "    max_entry = 0\n",
    "    for entry in coeffs:\n",
    "        if max(entry) > max_entry:\n",
    "            max_entry = max(entry)\n",
    "    return max_entry\n",
    "def filter_data(data, max_coeff=100, species_counts=[1,2,3,4,5], mandatory_species=[]):\n",
    "    filtered_data = []\n",
    "    for entry in data:\n",
    "        # check coefficients\n",
    "        try:\n",
    "            if get_max_coeff(get_coeffs([entry])) > max_coeff:\n",
    "                continue\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "        # check length\n",
    "        if len(mg.core.Composition(entry).elements) not in species_counts:\n",
    "            continue\n",
    "\n",
    "        # check that all species are from species_in use\n",
    "        if False in [species in species_in_use for species in list(mg.core.Composition(entry).as_dict().keys())]:\n",
    "            continue\n",
    "\n",
    "        # mandatory species\n",
    "        mandatory_count = len(mandatory_species)\n",
    "        mandatory_present = 0\n",
    "        if mandatory_count > 0:\n",
    "            for species in entry:\n",
    "                if species in mandatory_species:\n",
    "                    mandatory_present += 1\n",
    "            if mandatory_present != mandatory_count:\n",
    "                continue\n",
    "\n",
    "        filtered_data.append(entry)\n",
    "\n",
    "    return filtered_data\n",
    "\n",
    "def SynthNN_best_model(x_input,y_input,data_input):\n",
    "    #predicts the synthesizability using the best performing SynthNN model\n",
    "    num_positive=41599\n",
    "    tf.compat.v1.disable_eager_execution()\n",
    "    M =30\n",
    "    DIR='All_hyperparam_training/' +str(M) + 'M/20/'\n",
    "    name='performance_matrix_TL_v3_30M_14112.txt'\n",
    "\n",
    "    hyperparams=[name[-9],name[-8],name[-7],name[-6],name[-5]]\n",
    "    hyperparams=np.array(hyperparams, dtype=int)\n",
    "    no_h1=[30,40,50,60,80][hyperparams[1]]\n",
    "    no_h2=[30,40,50,60,80][hyperparams[2]]\n",
    "    x = tf.compat.v1.placeholder(tf.float32, shape=[None, x_input.shape[1]])\n",
    "    y_ = tf.compat.v1.placeholder(tf.float32, shape=[None, 2])\n",
    "    W1=tf.compat.v1.placeholder(tf.float32, shape=[x_input.shape[1],M]) #\n",
    "    F1 = tf.compat.v1.placeholder(tf.float32, shape=[M,no_h1])\n",
    "    F2 = tf.compat.v1.placeholder(tf.float32, shape=[no_h1,no_h2])\n",
    "    F3 = tf.compat.v1.placeholder(tf.float32, shape=[no_h2,2])\n",
    "    b1 = tf.compat.v1.placeholder(tf.float32, shape=[no_h1])\n",
    "    b2 = tf.compat.v1.placeholder(tf.float32, shape=[no_h2])\n",
    "    b3 = tf.compat.v1.placeholder(tf.float32, shape=[2])\n",
    "    sess = tf.compat.v1.InteractiveSession()\n",
    "    z0_raw = tf.multiply(tf.expand_dims(x,2),tf.expand_dims(W1,0)) #(ntr, I, M)\n",
    "    tempmean,var = tf.nn.moments(x=z0_raw,axes=[1])\n",
    "    z0 = tf.concat([tf.reduce_sum(input_tensor=z0_raw,axis=1)],1) #(ntr, M)\n",
    "    z1 = tf.add(tf.matmul(z0,F1),b1) #(ntr, no_h1)\n",
    "    a1 = tf.tanh(z1) #(ntr, no_h1)\n",
    "    z2= tf.add(tf.matmul(a1,F2),b2) #(ntr,no_h1)\n",
    "    a2= tf.tanh(z2) #(ntr, no_h1)\n",
    "    z3 = tf.add(tf.matmul(a2,F3),b3) #(ntr, 2)\n",
    "    a3 = tf.nn.softmax(z3) #(ntr, 2)\n",
    "    clipped_y = tf.clip_by_value(a3, 1e-10, 1.0)\n",
    "    cross_entropy = -tf.reduce_sum(input_tensor=y_*tf.math.log(clipped_y)*np.array([weight_for_1,weight_for_0]))\n",
    "    correct_prediction = tf.equal(tf.argmax(input=a3,axis=1), tf.argmax(input=y_,axis=1))\n",
    "    accuracy = tf.reduce_mean(input_tensor=tf.cast(correct_prediction, tf.float32))\n",
    "    sess.run(tf.compat.v1.initialize_all_variables())\n",
    "    model_name=str(M) + 'M_synth_v3_semi' + str(hyperparams[0]) + str(hyperparams[1])+ str(hyperparams[2])+ str(hyperparams[3])+ str(hyperparams[4])  +'.txt'\n",
    "    directory=DIR + '/'\n",
    "    W1_loaded=np.loadtxt(directory + 'W1_' + model_name) \n",
    "    F1_loaded=np.loadtxt(directory + 'F1_' + model_name) \n",
    "    F2_loaded=np.loadtxt(directory + 'F2_' + model_name) \n",
    "    F3_loaded=np.loadtxt(directory + 'F3_' + model_name) \n",
    "    F3_loaded=np.reshape(F3_loaded, [no_h2,2])\n",
    "    b1_loaded=np.loadtxt(directory + 'b1_' + model_name) \n",
    "    b2_loaded=np.loadtxt(directory + 'b2_' + model_name) \n",
    "    b3_loaded=np.loadtxt(directory + 'b3_' + model_name)\n",
    "    b3_loaded=np.reshape(b3_loaded, [2])\n",
    "    preds=a3.eval(feed_dict={x: x_input, y_: y_input , W1:W1_loaded, F1:F1_loaded, F2:F2_loaded, F3:F3_loaded, b1:b1_loaded, b2:b2_loaded, b3:b3_loaded})\n",
    "    #te_accuracy=accuracy.eval(feed_dict={x: x_input, y_: y_input , W1:W1_loaded, F1:F1_loaded, F2:F2_loaded, F3:F3_loaded, b1:b1_loaded, b2:b2_loaded, b3:b3_loaded})\n",
    "    sess.close()\n",
    "    return(preds)\n",
    "\n",
    "def get_model_preds(M,nsynth_train,hyperparameter_string,formulas):\n",
    "    #given input hyperparameters, M, nsynth_train and hyperparameter string, returns the predicted synthesizability\n",
    "    #formulas: array of input formulas string, i.e. ['NaCl','K2O']\n",
    "    DIR='All_hyperparam_training/' +str(M) + 'M/' + str(nsynth_train) +'/'\n",
    "    x_data=get_features(formulas)\n",
    "    y_data=np.zeros((len(formulas),2)) #make fake y_data\n",
    "    for name in os.listdir(DIR):\n",
    "        if(name.endswith('.txt')):\n",
    "            if(name.startswith('performance_matrix_TL_v3_' + str(M) + 'M_' + hyperparameter_string)):\n",
    "                hyperparams=[name[-9],name[-8],name[-7],name[-6],name[-5]]\n",
    "                hyperparams=np.array(hyperparams, dtype=int)\n",
    "                no_h1=[30,40,50,60,80][hyperparams[1]]\n",
    "                no_h2=[30,40,50,60,80][hyperparams[2]]\n",
    "                x = tf.compat.v1.placeholder(tf.float32, shape=[None, x_data.shape[1]])\n",
    "                y_ = tf.compat.v1.placeholder(tf.float32, shape=[None, 2])\n",
    "                W1=tf.compat.v1.placeholder(tf.float32, shape=[x_data.shape[1],M]) #if loading in weights for TL\n",
    "                F1 = tf.compat.v1.placeholder(tf.float32, shape=[M,no_h1])\n",
    "                F2 = tf.compat.v1.placeholder(tf.float32, shape=[no_h1,no_h2])\n",
    "                F3 = tf.compat.v1.placeholder(tf.float32, shape=[no_h2,2])\n",
    "                b1 = tf.compat.v1.placeholder(tf.float32, shape=[no_h1])\n",
    "                b2 = tf.compat.v1.placeholder(tf.float32, shape=[no_h2])\n",
    "                b3 = tf.compat.v1.placeholder(tf.float32, shape=[2])\n",
    "                sess = tf.compat.v1.InteractiveSession()\n",
    "                sess.run(tf.compat.v1.initialize_all_variables())\n",
    "                z0_raw = tf.multiply(tf.expand_dims(x,2),tf.expand_dims(W1,0)) #(ntr, I, M)\n",
    "                tempmean,var = tf.nn.moments(x=z0_raw,axes=[1])\n",
    "                z0 = tf.concat([tf.reduce_sum(input_tensor=z0_raw,axis=1)],1) #(ntr, M)\n",
    "                z1 = tf.add(tf.matmul(z0,F1),b1) #(ntr, no_h1)\n",
    "                a1 = tf.tanh(z1) #(ntr, no_h1)\n",
    "                z2= tf.add(tf.matmul(a1,F2),b2) #(ntr,no_h1)\n",
    "                a2= tf.tanh(z2) #(ntr, no_h1)\n",
    "                z3 = tf.add(tf.matmul(a2,F3),b3) #(ntr, 2)\n",
    "                a3 = tf.nn.softmax(z3) #(ntr, 2)\n",
    "                clipped_y = tf.clip_by_value(a3, 1e-10, 1.0)\n",
    "                cross_entropy = -tf.reduce_sum(input_tensor=y_*tf.math.log(clipped_y)*np.array([weight_for_1,weight_for_0]))\n",
    "                correct_prediction = tf.equal(tf.argmax(input=a3,axis=1), tf.argmax(input=y_,axis=1))\n",
    "                accuracy = tf.reduce_mean(input_tensor=tf.cast(correct_prediction, tf.float32))\n",
    "                sess.run(tf.compat.v1.initialize_all_variables())\n",
    "                model_name=str(M) + 'M_synth_v3_semi' + str(hyperparams[0]) + str(hyperparams[1])+ str(hyperparams[2])+ str(hyperparams[3])+ str(hyperparams[4])  +'.txt'\n",
    "                directory=DIR + '/'\n",
    "                W1_loaded=np.loadtxt(directory + 'W1_' + model_name) #load weights if doing TL HERE!!\n",
    "                F1_loaded=np.loadtxt(directory + 'F1_' + model_name) #load weights if doing TL HERE!!\n",
    "                F2_loaded=np.loadtxt(directory + 'F2_' + model_name) #load weights if doing TL HERE!!\n",
    "                F3_loaded=np.loadtxt(directory + 'F3_' + model_name) #load weights if doing TL HERE!!\n",
    "                F3_loaded=np.reshape(F3_loaded, [no_h2,2])\n",
    "                b1_loaded=np.loadtxt(directory + 'b1_' + model_name) #load weights if doing TL HERE!!\n",
    "                b2_loaded=np.loadtxt(directory + 'b2_' + model_name) #load weights if doing TL HERE!!\n",
    "                b3_loaded=np.loadtxt(directory + 'b3_' + model_name) #load weights if doing TL HERE!!\n",
    "                b3_loaded=np.reshape(b3_loaded, [2])\n",
    "                sess.run(tf.compat.v1.initialize_all_variables())\n",
    "                preds=a3.eval(feed_dict={x: x_data, y_: y_data , W1:W1_loaded, F1:F1_loaded, F2:F2_loaded, F3:F3_loaded, b1:b1_loaded, b2:b2_loaded, b3:b3_loaded})\n",
    "                sess.close()\n",
    "                return(preds)\n",
    "            \n",
    "def get_decade_model_preds(decade,formulas):\n",
    "    #given input hyperparameters, M, nsynth_train and hyperparameter string, returns the predicted synthesizability\n",
    "    #formulas: array of input formulas string, i.e. ['NaCl','K2O']\n",
    "    nsynth_train=20\n",
    "    M=30\n",
    "    hyperparameter_string='14112'\n",
    "    DIR='All_hyperparam_training/'+decade+ '_best/'\n",
    "    x_data=get_features(formulas)\n",
    "    y_data=np.zeros((len(formulas),2)) #make fake y_data\n",
    "    name='performance_matrix_TL_v3_' + str(M) + 'M_' + hyperparameter_string + '.txt'\n",
    "    hyperparams=[name[-9],name[-8],name[-7],name[-6],name[-5]]\n",
    "    hyperparams=np.array(hyperparams, dtype=int)\n",
    "    no_h1=[30,40,50,60,80][hyperparams[1]]\n",
    "    no_h2=[30,40,50,60,80][hyperparams[2]]\n",
    "    x = tf.compat.v1.placeholder(tf.float32, shape=[None, x_data.shape[1]])\n",
    "    y_ = tf.compat.v1.placeholder(tf.float32, shape=[None, 2])\n",
    "    W1=tf.compat.v1.placeholder(tf.float32, shape=[x_data.shape[1],M]) #if loading in weights for TL\n",
    "    F1 = tf.compat.v1.placeholder(tf.float32, shape=[M,no_h1])\n",
    "    F2 = tf.compat.v1.placeholder(tf.float32, shape=[no_h1,no_h2])\n",
    "    F3 = tf.compat.v1.placeholder(tf.float32, shape=[no_h2,2])\n",
    "    b1 = tf.compat.v1.placeholder(tf.float32, shape=[no_h1])\n",
    "    b2 = tf.compat.v1.placeholder(tf.float32, shape=[no_h2])\n",
    "    b3 = tf.compat.v1.placeholder(tf.float32, shape=[2])\n",
    "    sess = tf.compat.v1.InteractiveSession()\n",
    "    z0_raw = tf.multiply(tf.expand_dims(x,2),tf.expand_dims(W1,0)) #(ntr, I, M)\n",
    "    tempmean,var = tf.nn.moments(x=z0_raw,axes=[1])\n",
    "    z0 = tf.concat([tf.reduce_sum(input_tensor=z0_raw,axis=1)],1) #(ntr, M)\n",
    "    z1 = tf.add(tf.matmul(z0,F1),b1) #(ntr, no_h1)\n",
    "    a1 = tf.tanh(z1) #(ntr, no_h1)\n",
    "    z2= tf.add(tf.matmul(a1,F2),b2) #(ntr,no_h1)\n",
    "    a2= tf.tanh(z2) #(ntr, no_h1)\n",
    "    z3 = tf.add(tf.matmul(a2,F3),b3) #(ntr, 2)\n",
    "    a3 = tf.nn.softmax(z3) #(ntr, 2)\n",
    "    clipped_y = tf.clip_by_value(a3, 1e-10, 1.0)\n",
    "    cross_entropy = -tf.reduce_sum(input_tensor=y_*tf.math.log(clipped_y)*np.array([weight_for_1,weight_for_0]))\n",
    "    correct_prediction = tf.equal(tf.argmax(input=a3,axis=1), tf.argmax(input=y_,axis=1))\n",
    "    accuracy = tf.reduce_mean(input_tensor=tf.cast(correct_prediction, tf.float32))\n",
    "    model_name=str(M) + 'M_synth_v3_semi' + str(hyperparams[0]) + str(hyperparams[1])+ str(hyperparams[2])+ str(hyperparams[3])+ str(hyperparams[4])  +'.txt'\n",
    "    W1_loaded=np.loadtxt(DIR + 'W1_' + model_name) \n",
    "    F1_loaded=np.loadtxt(DIR + 'F1_' + model_name)\n",
    "    F2_loaded=np.loadtxt(DIR + 'F2_' + model_name) \n",
    "    F3_loaded=np.loadtxt(DIR + 'F3_' + model_name) \n",
    "    F3_loaded=np.reshape(F3_loaded, [no_h2,2])\n",
    "    b1_loaded=np.loadtxt(DIR + 'b1_' + model_name) \n",
    "    b2_loaded=np.loadtxt(DIR + 'b2_' + model_name) \n",
    "    b3_loaded=np.loadtxt(DIR + 'b3_' + model_name) \n",
    "    b3_loaded=np.reshape(b3_loaded, [2])\n",
    "    sess.run(tf.compat.v1.initialize_all_variables())\n",
    "    preds=a3.eval(feed_dict={x: x_data, y_: y_data , W1:W1_loaded, F1:F1_loaded, F2:F2_loaded, F3:F3_loaded, b1:b1_loaded, b2:b2_loaded, b3:b3_loaded})\n",
    "    sess.close()\n",
    "    return(preds)\n",
    "                     \n",
    "def get_block_performance(data,yvalues):\n",
    "    #only used in making Figure 3c\n",
    "    #inputs:\n",
    "    #data: array of formulas to make predictions on\n",
    "    #yvalues: ground truth labels of the formulas in data\n",
    "    #returns: the F1-scores of the s_block,p_block,d_block and f_block containing formulas\n",
    "\n",
    "    s_block_performance=0\n",
    "    p_block_performance=0\n",
    "    d_block_performance=0\n",
    "    f_block_performance=0\n",
    "    synthNN_TP_block_pred_dict = {\"s\": 0,\"p\": 0,\"d\": 0,\"f\":0}\n",
    "    synthNN_FP_block_pred_dict = {\"s\": 0,\"p\": 0,\"d\": 0,\"f\":0}\n",
    "    synthNN_TN_block_pred_dict = {\"s\": 0,\"p\": 0,\"d\": 0,\"f\":0}\n",
    "    synthNN_FN_block_pred_dict = {\"s\": 0,\"p\": 0,\"d\": 0,\"f\":0}\n",
    "    x_data=get_features(data)\n",
    "    dummy_y_values=np.zeros((len(data),2))\n",
    "    synthNN_quiz_preds=SynthNN_best_model(x_data,dummy_y_values,data)[:,0]\n",
    "\n",
    "    for i in range(len(data)):\n",
    "        elements=mg.core.composition.Composition(data[i]).as_dict().keys()\n",
    "        blocks_in_formula=np.unique([mg.core.periodic_table.Element(element).block for element in elements])\n",
    "        for element_block_name in blocks_in_formula:\n",
    "            if(yvalues[i]==1):\n",
    "                if(synthNN_quiz_preds[i]>0.5):\n",
    "                    synthNN_TP_block_pred_dict[element_block_name]=synthNN_TP_block_pred_dict[element_block_name]+1\n",
    "                else:\n",
    "                    synthNN_FN_block_pred_dict[element_block_name]=synthNN_FN_block_pred_dict[element_block_name]+1\n",
    "            else:\n",
    "                if(synthNN_quiz_preds[i]<0.5):\n",
    "                    synthNN_TN_block_pred_dict[element_block_name]=synthNN_TN_block_pred_dict[element_block_name]+1\n",
    "                else:\n",
    "                    synthNN_FP_block_pred_dict[element_block_name]=synthNN_FP_block_pred_dict[element_block_name]+1\n",
    "\n",
    "    #throw exception in case of no instances of that block\n",
    "    try:\n",
    "        s_block_performance=(synthNN_TP_block_pred_dict['s']/(synthNN_TP_block_pred_dict['s']+(0.5*(synthNN_FP_block_pred_dict['s']+synthNN_FN_block_pred_dict['s']))))\n",
    "    except ZeroDivisionError:\n",
    "        pass\n",
    "    try:\n",
    "        p_block_performance=(synthNN_TP_block_pred_dict['p']/(synthNN_TP_block_pred_dict['p']+(0.5*(synthNN_FP_block_pred_dict['p']+synthNN_FN_block_pred_dict['p']))))\n",
    "    except ZeroDivisionError:\n",
    "        pass\n",
    "    try:\n",
    "        d_block_performance=(synthNN_TP_block_pred_dict['d']/(synthNN_TP_block_pred_dict['d']+(0.5*(synthNN_FP_block_pred_dict['d']+synthNN_FN_block_pred_dict['d']))))\n",
    "    except ZeroDivisionError:\n",
    "        pass\n",
    "    try:\n",
    "        f_block_performance=(synthNN_TP_block_pred_dict['f']/(synthNN_TP_block_pred_dict['f']+(0.5*(synthNN_FP_block_pred_dict['f']+synthNN_FN_block_pred_dict['f']))))\n",
    "    except ZeroDivisionError:\n",
    "        pass\n",
    "    return(s_block_performance,p_block_performance,d_block_performance,f_block_performance)\n",
    "\n",
    "def get_element_type_color(symbol):\n",
    "    #given input symbol, return color to use in plot for Figure 5b\n",
    "    c=''\n",
    "\n",
    "    if(mg.core.periodic_table.Element(symbol).is_transition_metal):\n",
    "        return('purple')\n",
    "    elif(symbol=='O'):\n",
    "        return('red')\n",
    "    elif(mg.core.periodic_table.Element(symbol).is_post_transition_metal):\n",
    "        c='orange'\n",
    "    elif(mg.core.periodic_table.Element(symbol).is_rare_earth_metal):\n",
    "        c='green'\n",
    "    elif(symbol=='As' or symbol=='N' or symbol=='P'):\n",
    "        return('blue')\n",
    "    elif(mg.core.periodic_table.Element(symbol).is_metalloid):\n",
    "        c='cyan'\n",
    "    elif(mg.core.periodic_table.Element(symbol).is_halogen):\n",
    "        c='pink'\n",
    "    elif(mg.core.periodic_table.Element(symbol).is_chalcogen):\n",
    "        c='yellow'\n",
    "    elif(symbol=='C'):\n",
    "        c='gray'\n",
    "    elif(mg.core.periodic_table.Element(symbol).is_alkali):\n",
    "        c='yellow'\n",
    "    else:\n",
    "        c='white'\n",
    "    return(c)\n",
    "\n",
    "def get_batch_val_2000(neg_positive_ratio):\n",
    "    random.seed(3)\n",
    "    np.random.seed(3)\n",
    "    noTr_positives=30400 #number positive examples in train set\n",
    "    noTr_negatives=noTr_positives*neg_positive_ratio #no. negatives examples in train set\n",
    "    noTr = noTr_positives + (noTr_negatives) #total size of train set\n",
    "\n",
    "    #only sample from first 90% of dataset; shuffle first\n",
    "    data1=[]\n",
    "    f=open('All_hyperparam_training/icsd_full_data_unique_no_frac_2000_nopenta.txt')\n",
    "    i=0\n",
    "    for line in f:\n",
    "        if(i>noTr_positives and i<noTr_positives*1.05):\n",
    "            data1.append(line.replace('\\n',''))\n",
    "        i+=1\n",
    "    f.close()\n",
    "\n",
    "    data0=[]\n",
    "    f=open('All_hyperparam_training/standard_neg_ex_tr_val_v5_balanced_shuffled.txt')\n",
    "    i=0\n",
    "    for line in f:\n",
    "        if(i>noTr_negatives and i<noTr_negatives*1.05):\n",
    "            data0.append(line.replace('\\n',''))\n",
    "        i+=1\n",
    "    f.close()\n",
    "\n",
    "    #shuffle the positive and negative examples with themselves\n",
    "    negative_indices=list(range(0,len(data0)))\n",
    "    random.shuffle(negative_indices)\n",
    "    positive_indices=list(range(0,len(data1)))\n",
    "    random.shuffle(positive_indices)\n",
    "    data0=np.array(data0)\n",
    "    data1=np.array(data1)\n",
    "    data0=data0[negative_indices]\n",
    "    data1=data1[positive_indices]\n",
    "    featmat0=get_features(data0)\n",
    "    featmat1=get_features(data1)\n",
    "\n",
    "    #get labels\n",
    "    labs = np.zeros((len(data0) + len(data1),1))\n",
    "    for ind,ent in enumerate(data1):\n",
    "        labs[ind,0] = 1\n",
    "    unique, counts = np.unique(labs, return_counts=True)\n",
    "    ind0 = np.where(labs==0)[0] #indices of label=0\n",
    "    ind1 = np.where(labs==1)[0] #indices of label=1\n",
    "    #print(len(ind0),len(ind1))\n",
    "\n",
    "    #combine positives and negatives and shuffle\n",
    "\n",
    "    featmat3 = np.concatenate((featmat0,featmat1)) #set legths of labels 0 and 1 to be the same in the new feature matrix featmat3\n",
    "    datasorted = np.concatenate((data0,data1)) #data ordered the same as featmat3\n",
    "    labs3 = np.concatenate((labs[ind0], labs[ind1]), axis=0) #labels ordered the same as featmat3\n",
    "    noS = len(featmat3)\n",
    "    ind = list(range(0,noS)) #training set index\n",
    "    random.shuffle(ind) #shuffle training set index\n",
    "    #indB = list(range(0,noTr_subset)) #used later for batch\n",
    "    labs3 = np.column_stack((labs3,np.abs(labs3-1)))\n",
    "    xtr_batch = featmat3[ind[0:],:]\n",
    "    ytr_batch = labs3[ind[0:],:]\n",
    "    data_batch=datasorted[ind[0:]]\n",
    "    return(xtr_batch, ytr_batch, data_batch)\n",
    "\n",
    "def get_batch_val_2010(neg_positive_ratio):\n",
    "\n",
    "    random.seed(3)\n",
    "    np.random.seed(3)\n",
    "    noTr_positives=38900 #number positive examples in train set\n",
    "    noTr_negatives=noTr_positives*neg_positive_ratio #no. negatives examples in train set\n",
    "    noTr = noTr_positives + (noTr_negatives) #total size of train set\n",
    "\n",
    "    #only sample from first 90% of dataset; shuffle first\n",
    "    data1=[]\n",
    "    f=open('All_hyperparam_training/icsd_full_data_unique_no_frac_2010_nopenta.txt')\n",
    "    i=0\n",
    "    for line in f:\n",
    "        if(i>noTr_positives and i<noTr_positives*1.05):\n",
    "            data1.append(line.replace('\\n',''))\n",
    "        i+=1\n",
    "    f.close()\n",
    "\n",
    "    data0=[]\n",
    "    f=open('All_hyperparam_training/standard_neg_ex_tr_val_v5_balanced_shuffled.txt')\n",
    "    i=0\n",
    "    for line in f:\n",
    "        if(i>noTr_negatives and i<noTr_negatives*1.05):\n",
    "            data0.append(line.replace('\\n',''))\n",
    "        i+=1\n",
    "    f.close()\n",
    "\n",
    "    #shuffle the positive and negative examples with themselves\n",
    "    negative_indices=list(range(0,len(data0)))\n",
    "    random.shuffle(negative_indices)\n",
    "    positive_indices=list(range(0,len(data1)))\n",
    "    random.shuffle(positive_indices)\n",
    "    data0=np.array(data0)\n",
    "    data1=np.array(data1)\n",
    "    data0=data0[negative_indices]\n",
    "    data1=data1[positive_indices]\n",
    "    featmat0=get_features(data0)\n",
    "    featmat1=get_features(data1)\n",
    "\n",
    "    #get labels\n",
    "    labs = np.zeros((len(data0) + len(data1),1))\n",
    "    for ind,ent in enumerate(data1):\n",
    "        labs[ind,0] = 1\n",
    "    unique, counts = np.unique(labs, return_counts=True)\n",
    "    ind0 = np.where(labs==0)[0] #indices of label=0\n",
    "    ind1 = np.where(labs==1)[0] #indices of label=1\n",
    "    #print(len(ind0),len(ind1))\n",
    "\n",
    "    #combine positives and negatives and shuffle\n",
    "    featmat3 = np.concatenate((featmat0,featmat1)) #set legths of labels 0 and 1 to be the same in the new feature matrix featmat3\n",
    "    datasorted = np.concatenate((data0,data1)) #data ordered the same as featmat3\n",
    "    labs3 = np.concatenate((labs[ind0], labs[ind1]), axis=0) #labels ordered the same as featmat3\n",
    "    noS = len(featmat3)\n",
    "    ind = list(range(0,noS)) #training set index\n",
    "    random.shuffle(ind) #shuffle training set index\n",
    "    #indB = list(range(0,noTr_subset)) #used later for batch\n",
    "    labs3 = np.column_stack((labs3,np.abs(labs3-1)))\n",
    "    xtr_batch = featmat3[ind[0:],:]\n",
    "    ytr_batch = labs3[ind[0:],:]\n",
    "    data_batch=datasorted[ind[0:]]\n",
    "    return(xtr_batch, ytr_batch, data_batch)\n",
    "\n",
    "def get_batch_val_1990(neg_positive_ratio):\n",
    "\n",
    "    random.seed(3)\n",
    "    np.random.seed(3)\n",
    "    noTr_positives=22600 #number positive examples in train set\n",
    "    noTr_negatives=noTr_positives*neg_positive_ratio #no. negatives examples in train set\n",
    "    noTr = noTr_positives + (noTr_negatives) #total size of train set\n",
    "\n",
    "    #only sample from first 90% of dataset; shuffle first\n",
    "    data1=[]\n",
    "    f=open('All_hyperparam_training/icsd_full_data_unique_no_frac_1990_nopenta.txt')\n",
    "    i=0\n",
    "    for line in f:\n",
    "        if(i>noTr_positives and i<noTr_positives*1.05):\n",
    "            data1.append(line.replace('\\n',''))\n",
    "        i+=1\n",
    "    f.close()\n",
    "\n",
    "    data0=[]\n",
    "    f=open('All_hyperparam_training/standard_neg_ex_tr_val_v5_balanced_shuffled.txt')\n",
    "    i=0\n",
    "    for line in f:\n",
    "        if(i>noTr_negatives and i<noTr_negatives*1.05):\n",
    "            data0.append(line.replace('\\n',''))\n",
    "        i+=1\n",
    "    f.close()\n",
    "\n",
    "    #shuffle the positive and negative examples with themselves\n",
    "    negative_indices=list(range(0,len(data0)))\n",
    "    random.shuffle(negative_indices)\n",
    "    positive_indices=list(range(0,len(data1)))\n",
    "    random.shuffle(positive_indices)\n",
    "    data0=np.array(data0)\n",
    "    data1=np.array(data1)\n",
    "    data0=data0[negative_indices]\n",
    "    data1=data1[positive_indices]\n",
    "    featmat0=get_features(data0)\n",
    "    featmat1=get_features(data1)\n",
    "\n",
    "    #get labels\n",
    "    labs = np.zeros((len(data0) + len(data1),1))\n",
    "    for ind,ent in enumerate(data1):\n",
    "        labs[ind,0] = 1\n",
    "    unique, counts = np.unique(labs, return_counts=True)\n",
    "    ind0 = np.where(labs==0)[0] #indices of label=0\n",
    "    ind1 = np.where(labs==1)[0] #indices of label=1\n",
    "    #print(len(ind0),len(ind1))\n",
    "\n",
    "    #combine positives and negatives and shuffle\n",
    "    featmat3 = np.concatenate((featmat0,featmat1)) #set legths of labels 0 and 1 to be the same in the new feature matrix featmat3\n",
    "    datasorted = np.concatenate((data0,data1)) #data ordered the same as featmat3\n",
    "    labs3 = np.concatenate((labs[ind0], labs[ind1]), axis=0) #labels ordered the same as featmat3\n",
    "    noS = len(featmat3)\n",
    "    ind = list(range(0,noS)) #training set index\n",
    "    random.shuffle(ind) #shuffle training set index\n",
    "    #indB = list(range(0,noTr_subset)) #used later for batch\n",
    "    labs3 = np.column_stack((labs3,np.abs(labs3-1)))\n",
    "    xtr_batch = featmat3[ind[0:],:]\n",
    "    ytr_batch = labs3[ind[0:],:]\n",
    "    data_batch=datasorted[ind[0:]]\n",
    "    return(xtr_batch, ytr_batch, data_batch)\n",
    "\n",
    "def get_batch_val_1980(neg_positive_ratio):\n",
    "\n",
    "    random.seed(3)\n",
    "    np.random.seed(3)\n",
    "    noTr_positives=15100 #number positive examples in train set\n",
    "    noTr_negatives=noTr_positives*neg_positive_ratio #no. negatives examples in train set\n",
    "    noTr = noTr_positives + (noTr_negatives) #total size of train set\n",
    "\n",
    "    #only sample from first 90% of dataset; shuffle first\n",
    "    data1=[]\n",
    "    f=open('All_hyperparam_training/icsd_full_data_unique_no_frac_1980_nopenta.txt')\n",
    "    i=0\n",
    "    for line in f:\n",
    "        if(i>noTr_positives and i<noTr_positives*1.05):\n",
    "            data1.append(line.replace('\\n',''))\n",
    "        i+=1\n",
    "    f.close()\n",
    "    data0=[]\n",
    "    f=open('All_hyperparam_training/standard_neg_ex_tr_val_v5_balanced_shuffled.txt')\n",
    "    i=0\n",
    "    for line in f:\n",
    "        if(i>noTr_negatives and i<noTr_negatives*1.05):\n",
    "            data0.append(line.replace('\\n',''))\n",
    "        i+=1\n",
    "    f.close()\n",
    "\n",
    "    #shuffle the positive and negative examples with themselves\n",
    "    negative_indices=list(range(0,len(data0)))\n",
    "    random.shuffle(negative_indices)\n",
    "    positive_indices=list(range(0,len(data1)))\n",
    "    random.shuffle(positive_indices)\n",
    "    data0=np.array(data0)\n",
    "    data1=np.array(data1)\n",
    "    data0=data0[negative_indices]\n",
    "    data1=data1[positive_indices]\n",
    "    featmat0=get_features(data0)\n",
    "    featmat1=get_features(data1)\n",
    "\n",
    "    #get labels\n",
    "    labs = np.zeros((len(data0) + len(data1),1))\n",
    "    for ind,ent in enumerate(data1):\n",
    "        labs[ind,0] = 1\n",
    "    unique, counts = np.unique(labs, return_counts=True)\n",
    "    ind0 = np.where(labs==0)[0] #indices of label=0\n",
    "    ind1 = np.where(labs==1)[0] #indices of label=1\n",
    "    #print(len(ind0),len(ind1))\n",
    "\n",
    "    #combine positives and negatives and shuffle\n",
    "    featmat3 = np.concatenate((featmat0,featmat1)) #set legths of labels 0 and 1 to be the same in the new feature matrix featmat3\n",
    "    datasorted = np.concatenate((data0,data1)) #data ordered the same as featmat3\n",
    "    labs3 = np.concatenate((labs[ind0], labs[ind1]), axis=0) #labels ordered the same as featmat3\n",
    "    noS = len(featmat3)\n",
    "    ind = list(range(0,noS)) #training set index\n",
    "    random.shuffle(ind) #shuffle training set index\n",
    "    #indB = list(range(0,noTr_subset)) #used later for batch\n",
    "    labs3 = np.column_stack((labs3,np.abs(labs3-1)))\n",
    "    xtr_batch = featmat3[ind[0:],:]\n",
    "    ytr_batch = labs3[ind[0:],:]\n",
    "    data_batch=datasorted[ind[0:]]\n",
    "    return(xtr_batch, ytr_batch, data_batch)\n",
    "\n",
    "def get_ox_dict(dict_name):\n",
    "    #returns either the 'common' or 'full' oxidation state dictionary for all elements\n",
    "    species_in_use = ['Ac', 'Ag', 'Al', 'As', 'Au', 'B', 'Ba', 'Be', 'Bi', 'Br', 'C', 'Ca', 'Cd', 'Ce', 'Cl', 'Co', 'Cr', 'Cs', 'Cu', 'Dy', 'Er', 'Eu', 'F', 'Fe', 'Ga', 'Gd', 'Ge', 'H', 'Hf', 'Hg', 'Ho', 'I', 'In', 'Ir', 'K', 'La', 'Li', 'Lu', 'Mg', 'Mn', 'Mo', 'N', 'Na', 'Nb', 'Nd', 'Ni', 'Np', 'O', 'Os', 'P', 'Pa', 'Pb', 'Pd', 'Pr', 'Pt', 'Pu', 'Rb', 'Re', 'Rh', 'Ru', 'S', 'Sb', 'Sc', 'Se', 'Si', 'Sm', 'Sn', 'Sr', 'Ta', 'Tb', 'Te', 'Th', 'Ti', 'Tl', 'Tm', 'U', 'V', 'W', 'Y', 'Yb', 'Zn', 'Zr']\n",
    "    element_names_array=['H','He','Li','Be','B','C','N','O','F','Ne','Na','Mg','Al','Si','P','S','Cl', 'Ar','K','Ca','Sc','Ti','V','Cr','Mn','Fe','Co','Ni','Cu','Zn','Ga','Ge','As','Se','Br','Kr','Rb','Sr','Y','Zr','Nb','Mo','Tc','Ru','Rh','Pd','Ag','Cd','In','Sn','Sb','Te','I','Xe','Cs','Ba','La','Ce','Pr','Nd','Pm','Sm','Eu','Gd','Tb','Dy','Ho','Er','Tm','Yb','Lu','Hf','Ta','W','Re','Os','Ir','Pt','Au','Hg','Tl','Pb','Bi','Po','At','Rn','Fr','Ra','Ac','Th','Pa','U','Np','Pu']\n",
    "\n",
    "    if(dict_name=='common'):\n",
    "        mg_common_ox_dict = {}\n",
    "        for specie in species_in_use:\n",
    "            mg_common_ox_dict[specie] = list(mg.core.periodic_table.Specie(specie).common_oxidation_states)\n",
    "        common_ox_dict = mg_common_ox_dict\n",
    "        common_ox_dict = {x: common_ox_dict[x] for x in common_ox_dict if x in species_in_use}\n",
    "        return(common_ox_dict)\n",
    "    elif(dict_name=='full'):\n",
    "        mg_full_ox_dict = {}\n",
    "        for specie in species_in_use:\n",
    "            mg_full_ox_dict[specie] = list(mg.core.periodic_table.Specie(specie).oxidation_states)\n",
    "        full_ox_dict = mg_full_ox_dict\n",
    "        full_ox_dict = {x: full_ox_dict[x] for x in full_ox_dict if x in species_in_use}\n",
    "        return(full_ox_dict)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
